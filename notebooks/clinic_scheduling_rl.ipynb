{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heather306/test-repo/blob/cursor%2Ftrain-system-app-scheduling-with-reinforcement-learning-095a/notebooks/clinic_scheduling_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8cad0fc"
      },
      "source": [
        "# Clinic Scheduling via Reinforcement Learning (Gymnasium + Stable-Baselines3)\n",
        "\n",
        "This Colab-ready notebook builds and trains a PPO agent to schedule patients under clinic constraints:\n",
        "- Monday–Saturday only (Sunday closed)\n",
        "- Operating hours: 08:00–12:00 and 13:00–16:00\n",
        "- Lunch break 12:00–13:00 (no scheduling)\n",
        "- Max 60 scheduled patient slots per day\n",
        "- Walk-ins accepted until cutoff; excess wait in walk-in queue\n",
        "- If a scheduled patient is not on-site at their time, move to late list and serve next\n",
        "- If a late patient arrives later, admin can restore to original position; they get priority next after current patient\n",
        "\n",
        "We'll define a custom Gymnasium environment, train a PPO policy with Stable-Baselines3, and evaluate/visualize outcomes."
      ],
      "id": "b8cad0fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keep Notebook Synced With GitHub\n",
        "\n",
        "Working in Colab and want to make sure you are on the latest GitHub commit? Run the next cell. It will clone (or update) the repo branch into `/content/test-repo`, so you can reopen this notebook directly from the synced copy after it finishes."
      ],
      "id": "6f6e4f56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clone or pull the latest notebook/code from GitHub\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/Heather306/test-repo.git\"\n",
        "BRANCH = \"cursor/train-system-app-scheduling-with-reinforcement-learning-095a\"\n",
        "LOCAL_DIR = Path(\"/content/test-repo\")\n",
        "\n",
        "\n",
        "def run(cmd):\n",
        "    print(\">\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "\n",
        "# If the folder exists without a git repo, clear it so we can clone fresh\n",
        "if LOCAL_DIR.exists() and not (LOCAL_DIR / \".git\").exists():\n",
        "    shutil.rmtree(LOCAL_DIR)\n",
        "\n",
        "if (LOCAL_DIR / \".git\").exists():\n",
        "    run([\"git\", \"-C\", str(LOCAL_DIR), \"fetch\", \"origin\"])\n",
        "    run([\"git\", \"-C\", str(LOCAL_DIR), \"checkout\", BRANCH])\n",
        "    run([\"git\", \"-C\", str(LOCAL_DIR), \"pull\", \"origin\", BRANCH])\n",
        "else:\n",
        "    if LOCAL_DIR.exists():\n",
        "        shutil.rmtree(LOCAL_DIR)\n",
        "    run([\"git\", \"clone\", \"--branch\", BRANCH, REPO_URL, str(LOCAL_DIR)])\n",
        "\n",
        "nb_path = LOCAL_DIR / \"notebooks\" / \"clinic_scheduling_rl.ipynb\"\n",
        "print(f\"Latest notebook available at: {nb_path}\")\n",
        "print(\"In Colab, use File → Open notebook → Upload → Browse (left 'Files' pane) to reopen from that path if needed.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "86263f17"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dcb88bf",
        "outputId": "885d20b7-52d2-4271-a1a7-608347397087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# If running in Colab, uncomment the next line to install packages\n",
        "# !pip -q install gymnasium==0.29.1 stable-baselines3==2.3.2 sb3-contrib==2.3.2 shimmy==1.3.0 plotly==5.24.1 numpy pandas\n",
        "\n",
        "import sys, os\n",
        "print(sys.version)\n",
        "print(\"Working dir:\", os.getcwd())\n",
        "\n",
        "# Ensure proper imports in Colab kernels\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Working dir: /content\n"
          ]
        }
      ],
      "id": "8dcb88bf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48bab9e8"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, Any, Optional\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "# Domain constants\n",
        "MINUTES_OPEN_AM = 8 * 60\n",
        "MINUTES_LUNCH_START = 12 * 60\n",
        "MINUTES_LUNCH_END = 13 * 60\n",
        "MINUTES_CLOSE_PM = 16 * 60\n",
        "WORK_MINUTES = (MINUTES_LUNCH_START - MINUTES_OPEN_AM) + (MINUTES_CLOSE_PM - MINUTES_LUNCH_END)\n",
        "MAX_SCHEDULED_PER_DAY = 60\n",
        "MAX_WALKIN_QUEUE = 200\n",
        "DAYS_OPEN = set(range(6))  # 0=Mon ... 5=Sat, 6=Sun closed\n",
        "\n",
        "\n",
        "def is_open_minute(minute_of_day: int) -> bool:\n",
        "    return (MINUTES_OPEN_AM <= minute_of_day < MINUTES_LUNCH_START) or (MINUTES_LUNCH_END <= minute_of_day < MINUTES_CLOSE_PM)\n",
        "\n",
        "\n",
        "def minute_to_slot(minute_of_day: int, slot_minutes: int) -> int:\n",
        "    # Map minute to contiguous slot index excluding lunch\n",
        "    if minute_of_day < MINUTES_OPEN_AM:\n",
        "        return 0\n",
        "    if MINUTES_OPEN_AM <= minute_of_day < MINUTES_LUNCH_START:\n",
        "        return (minute_of_day - MINUTES_OPEN_AM) // slot_minutes\n",
        "    if MINUTES_LUNCH_START <= minute_of_day < MINUTES_LUNCH_END:\n",
        "        return (MINUTES_LUNCH_START - MINUTES_OPEN_AM) // slot_minutes\n",
        "    if MINUTES_LUNCH_END <= minute_of_day < MINUTES_CLOSE_PM:\n",
        "        am_slots = (MINUTES_LUNCH_START - MINUTES_OPEN_AM) // slot_minutes\n",
        "        return am_slots + (minute_of_day - MINUTES_LUNCH_END) // slot_minutes\n",
        "    return ((MINUTES_LUNCH_START - MINUTES_OPEN_AM) + (MINUTES_CLOSE_PM - MINUTES_LUNCH_END)) // slot_minutes\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Patient:\n",
        "    id: int\n",
        "    scheduled_slot: Optional[int]  # None for walk-in\n",
        "    arrival_time_min: Optional[int]  # None means not arrived yet\n",
        "    is_late: bool = False\n",
        "\n",
        "\n",
        "class ClinicSchedulingEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self,\n",
        "                 slot_minutes: int = 10,\n",
        "                 max_scheduled: int = MAX_SCHEDULED_PER_DAY,\n",
        "                 max_walkin_queue: int = MAX_WALKIN_QUEUE,\n",
        "                 no_show_prob: float = 0.05,\n",
        "                 late_prob: float = 0.1,\n",
        "                 walkin_rate_per_hour: float = 8.0,\n",
        "                 walkin_cutoff_minute: Optional[int] = None,\n",
        "                 day_of_week: Optional[int] = None,\n",
        "                 seed: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.slot_minutes = slot_minutes\n",
        "        self.slots_per_day = WORK_MINUTES // slot_minutes\n",
        "        self.max_scheduled = min(max_scheduled, MAX_SCHEDULED_PER_DAY)\n",
        "        self.max_walkin_queue = max_walkin_queue\n",
        "        self.no_show_prob = no_show_prob\n",
        "        self.late_prob = late_prob\n",
        "        self.walkin_rate_per_hour = walkin_rate_per_hour\n",
        "        self.walkin_cutoff_minute = walkin_cutoff_minute or MINUTES_CLOSE_PM\n",
        "        self._configured_day_of_week = day_of_week\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        # Action space: choose next source to serve\n",
        "        # 0 = next scheduled on-time, 1 = next walk-in, 2 = recall-priority late (if any)\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space (compact):\n",
        "        # [current_slot_index, scheduled_remaining, walkin_queue_len, late_list_len, next_scheduled_on_site(0/1), time_to_next_arrival_minutes]\n",
        "        high = np.array([\n",
        "            self.slots_per_day,\n",
        "            MAX_SCHEDULED_PER_DAY,\n",
        "            self.max_walkin_queue,\n",
        "            MAX_SCHEDULED_PER_DAY,\n",
        "            1,\n",
        "            60\n",
        "        ], dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=0.0, high=high, dtype=np.float32)\n",
        "\n",
        "        self.reset_state()\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.minute = MINUTES_OPEN_AM\n",
        "        self.current_slot = 0\n",
        "        # self.day_of_week is set in reset(); don't override here\n",
        "        self.scheduled: Dict[int, Patient] = {}\n",
        "        self.walkin_queue: list[Patient] = []\n",
        "        self.late_list: list[Patient] = []\n",
        "        self.served_ids: list[int] = []\n",
        "        self.served_log: list[Dict[str, Any]] = []\n",
        "        self.generated_patients: Dict[int, Patient] = {}\n",
        "        self._generate_day_schedule()\n",
        "\n",
        "    def _generate_day_schedule(self):\n",
        "        # Pre-generate scheduled patients across slots (max 60)\n",
        "        max_slots = self.slots_per_day\n",
        "        chosen_slots = self.rng.choice(max_slots, size=min(self.max_scheduled, max_slots), replace=False)\n",
        "        pid = 1\n",
        "        for slot in sorted(chosen_slots.tolist()):\n",
        "            # arrival: on time or late or no-show\n",
        "            ontime = self.rng.random() > self.late_prob\n",
        "            if self.rng.random() < self.no_show_prob:\n",
        "                arrival = None\n",
        "            else:\n",
        "                if ontime:\n",
        "                    # arrive within slot's first 5 minutes\n",
        "                    slot_minute = self._slot_to_minute(slot)\n",
        "                    jitter = int(self.rng.integers(0, min(5, self.slot_minutes)))\n",
        "                    arrival = slot_minute + jitter\n",
        "                else:\n",
        "                    # late: arrive between +5 and +60 minutes later\n",
        "                    base = self._slot_to_minute(slot) + 5\n",
        "                    arrival = min(base + int(self.rng.integers(0, 60)), MINUTES_CLOSE_PM - 1)\n",
        "            p = Patient(id=pid, scheduled_slot=slot, arrival_time_min=arrival)\n",
        "            self.scheduled[slot] = p\n",
        "            self.generated_patients[pid] = p\n",
        "            pid += 1\n",
        "        self.next_walkin_id = pid\n",
        "\n",
        "    def _slot_to_minute(self, slot_index: int) -> int:\n",
        "        am_slots = (MINUTES_LUNCH_START - MINUTES_OPEN_AM) // self.slot_minutes\n",
        "        if slot_index < am_slots:\n",
        "            return MINUTES_OPEN_AM + slot_index * self.slot_minutes\n",
        "        else:\n",
        "            return MINUTES_LUNCH_END + (slot_index - am_slots) * self.slot_minutes\n",
        "\n",
        "    def _poisson(self, lam):\n",
        "        # simple Poisson sampler via numpy\n",
        "        return self.rng.poisson(lam)\n",
        "\n",
        "    def _maybe_generate_walkins(self):\n",
        "        if not is_open_minute(self.minute):\n",
        "            return\n",
        "        if self.minute >= self.walkin_cutoff_minute:\n",
        "            return\n",
        "        # per-minute rate\n",
        "        lam = self.walkin_rate_per_hour / 60.0\n",
        "        arrivals = self._poisson(lam)\n",
        "        for _ in range(arrivals):\n",
        "            p = Patient(id=self.next_walkin_id, scheduled_slot=None, arrival_time_min=self.minute)\n",
        "            self.next_walkin_id += 1\n",
        "            if len(self.walkin_queue) < self.max_walkin_queue:\n",
        "                self.walkin_queue.append(p)\n",
        "                self.generated_patients[p.id] = p\n",
        "\n",
        "    def _update_late_status(self):\n",
        "        # mark scheduled as late if slot passed and not on-site\n",
        "        for slot, p in list(self.scheduled.items()):\n",
        "            slot_minute = self._slot_to_minute(slot)\n",
        "            if p.arrival_time_min is None:\n",
        "                continue  # no-show remains\n",
        "            if p.arrival_time_min > self.minute and self.minute >= slot_minute:\n",
        "                p.is_late = True\n",
        "                # move to late list if not already served and past slot\n",
        "                if p not in self.late_list and p.id not in self.served_ids and self.minute >= slot_minute:\n",
        "                    self.late_list.append(p)\n",
        "\n",
        "        # move any arrived late patients out of late_list priority bucket if they just arrived now\n",
        "        for p in self.late_list:\n",
        "            if p.arrival_time_min is not None and p.arrival_time_min <= self.minute:\n",
        "                p.is_late = True  # keep flag\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "        # set day-of-week (exclude Sunday by default)\n",
        "        if options and \"day_of_week\" in options:\n",
        "            self.day_of_week = int(options[\"day_of_week\"])\n",
        "        else:\n",
        "            if self._configured_day_of_week is not None:\n",
        "                self.day_of_week = int(self._configured_day_of_week)\n",
        "            else:\n",
        "                self.day_of_week = int(self.rng.integers(0, 6))\n",
        "                if self.day_of_week not in DAYS_OPEN:\n",
        "                    # force into open set\n",
        "                    self.day_of_week = int(self.rng.integers(0, 6))\n",
        "        # If Sunday (closed), start and immediately end episode by returning a terminal state\n",
        "        self.reset_state()\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # next scheduled slot that is not served yet\n",
        "        remaining_slots = [s for s, p in self.scheduled.items() if p.id not in self.served_ids]\n",
        "        scheduled_remaining = len(remaining_slots)\n",
        "\n",
        "        # check if the next scheduled patient is on-site now\n",
        "        next_slot = min(remaining_slots) if remaining_slots else None\n",
        "        on_site = 0\n",
        "        if next_slot is not None:\n",
        "            p = self.scheduled[next_slot]\n",
        "            if p.arrival_time_min is not None and p.arrival_time_min <= self.minute:\n",
        "                on_site = 1\n",
        "\n",
        "        # estimate time to next arrival (scheduled or walk-in), clipped\n",
        "        next_arrival = 999\n",
        "        # next scheduled arrival\n",
        "        sched_arrivals = [p.arrival_time_min for p in self.scheduled.values() if p.arrival_time_min is not None and p.arrival_time_min > self.minute]\n",
        "        if sched_arrivals:\n",
        "            next_arrival = min(next_arrival, min(sched_arrivals) - self.minute)\n",
        "        # approximate next walk-in arrival as inverse rate\n",
        "        if is_open_minute(self.minute) and self.walkin_rate_per_hour > 0 and self.minute < self.walkin_cutoff_minute:\n",
        "            expected_walkin_gap = max(1, int(60.0 / self.walkin_rate_per_hour))\n",
        "            next_arrival = min(next_arrival, expected_walkin_gap)\n",
        "        next_arrival = int(min(next_arrival, 60))\n",
        "\n",
        "        obs = np.array([\n",
        "            minute_to_slot(self.minute, self.slot_minutes),\n",
        "            scheduled_remaining,\n",
        "            len(self.walkin_queue),\n",
        "            len(self.late_list),\n",
        "            on_site,\n",
        "            next_arrival\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def step(self, action: int):\n",
        "        # simulate minute-by-minute until either we serve someone or day ends\n",
        "        reward = 0.0\n",
        "        info: Dict[str, Any] = {}\n",
        "\n",
        "        # At each step, first update arrivals\n",
        "        self._maybe_generate_walkins()\n",
        "        self._update_late_status()\n",
        "\n",
        "        # determine candidate queues based on action\n",
        "        # 2: late priority (admin recalls) if available\n",
        "        # 0: scheduled on-time if on-site\n",
        "        # 1: walk-in otherwise\n",
        "        served_patient: Optional[Patient] = None\n",
        "        served_source = None\n",
        "        served_via_recall = False\n",
        "\n",
        "        if action == 2 and self.late_list:\n",
        "            served_patient = self.late_list.pop(0)\n",
        "            served_source = \"late_recall\"\n",
        "            served_via_recall = True\n",
        "        else:\n",
        "            # next scheduled\n",
        "            remaining_slots = sorted([s for s, p in self.scheduled.items() if p.id not in self.served_ids])\n",
        "            if action == 0 and remaining_slots:\n",
        "                next_slot = remaining_slots[0]\n",
        "                p = self.scheduled[next_slot]\n",
        "                if p.arrival_time_min is not None and p.arrival_time_min <= self.minute:\n",
        "                    served_patient = p\n",
        "                    served_source = \"scheduled\"\n",
        "            if served_patient is None and self.walkin_queue:\n",
        "                served_patient = self.walkin_queue.pop(0)\n",
        "                served_source = \"walkin\"\n",
        "\n",
        "        # apply serving and time advance\n",
        "        if served_patient is not None:\n",
        "            self.served_ids.append(served_patient.id)\n",
        "            # reward per patient served; small bonus for serving scheduled/late via recall\n",
        "            reward += 1.0\n",
        "            if served_source in (\"scheduled\", \"late_recall\"):\n",
        "                reward += 0.05\n",
        "            # compute wait\n",
        "            served_start_minute = self.minute\n",
        "            arrival = served_patient.arrival_time_min\n",
        "            wait = None\n",
        "            if arrival is not None:\n",
        "                wait = max(0, served_start_minute - arrival)\n",
        "            # log\n",
        "            self.served_log.append({\n",
        "                \"id\": served_patient.id,\n",
        "                \"served_start_minute\": served_start_minute,\n",
        "                \"scheduled_slot\": served_patient.scheduled_slot,\n",
        "                \"arrival_time\": arrival,\n",
        "                \"is_walkin\": served_patient.scheduled_slot is None,\n",
        "                \"is_late\": bool(served_patient.is_late),\n",
        "                \"served_via_recall\": served_via_recall,\n",
        "                \"wait_minutes\": wait,\n",
        "                \"source\": served_source,\n",
        "            })\n",
        "            advance = self.slot_minutes\n",
        "        else:\n",
        "            # idle minute penalty for waiting while queues exist\n",
        "            advance = 1\n",
        "            reward -= 0.01\n",
        "\n",
        "        # advance time respecting lunch/closed periods\n",
        "        self.minute += advance\n",
        "        if MINUTES_LUNCH_START <= self.minute < MINUTES_LUNCH_END:\n",
        "            self.minute = MINUTES_LUNCH_END\n",
        "        done = self.minute >= MINUTES_CLOSE_PM\n",
        "\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        # small penalty for leaving late list unserved to encourage recalls\n",
        "        reward -= 0.001 * len(self.late_list)\n",
        "\n",
        "        # Encourage finishing scheduled by end of day\n",
        "        if done:\n",
        "            remaining_sched = len([p for p in self.scheduled.values() if p.id not in self.served_ids])\n",
        "            if remaining_sched > 0:\n",
        "                reward -= 0.1 * remaining_sched\n",
        "\n",
        "        return obs, reward, done, False, info\n",
        "\n",
        "    def render(self):\n",
        "        print({\n",
        "            \"time\": self.minute,\n",
        "            \"served\": len(self.served_ids),\n",
        "            \"walkin_q\": len(self.walkin_queue),\n",
        "            \"late\": len(self.late_list)\n",
        "        })"
      ],
      "execution_count": 3,
      "outputs": [],
      "id": "48bab9e8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMaqknLE9AsK"
      },
      "source": [
        "!pip -q install stable-baselines3==2.3.2 gymnasium==0.29.1 shimmy==1.3.0 plotly==5.24.1 ipywidgets"
      ],
      "execution_count": 7,
      "outputs": [],
      "id": "JMaqknLE9AsK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16de2fde",
        "outputId": "844c6576-7a6e-411e-8912-e52d996a343a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Quick smoke test of env dynamics (7-min slots)\n",
        "env = ClinicSchedulingEnv(slot_minutes=7, seed=42)\n",
        "obs, info = env.reset()\n",
        "print(\"obs shape:\", obs.shape, \"obs:\", obs)\n",
        "for _ in range(5):\n",
        "    a = env.action_space.sample()\n",
        "    obs, reward, terminated, truncated, info = env.step(a)\n",
        "    print({\"a\": int(a), \"r\": float(reward), \"done\": bool(terminated or truncated), \"obs\": obs.tolist()})"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obs shape: (6,) obs: [ 0. 60.  0.  0.  0.  1.]\n",
            "{'a': 2, 'r': 1.05, 'done': False, 'obs': [1.0, 59.0, 0.0, 0.0, 0.0, 7.0]}\n",
            "{'a': 1, 'r': -0.011, 'done': False, 'obs': [1.0, 59.0, 0.0, 1.0, 0.0, 7.0]}\n",
            "{'a': 1, 'r': -0.011, 'done': False, 'obs': [1.0, 59.0, 0.0, 1.0, 0.0, 7.0]}\n",
            "{'a': 0, 'r': 0.999, 'done': False, 'obs': [2.0, 59.0, 0.0, 1.0, 0.0, 7.0]}\n",
            "{'a': 2, 'r': 1.0490000000000002, 'done': False, 'obs': [3.0, 58.0, 0.0, 1.0, 0.0, 2.0]}\n"
          ]
        }
      ],
      "id": "16de2fde"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de8ad6a1",
        "outputId": "9bf6c2a2-8f04-4c79-bfd0-1908cd8ee682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Training: PPO on the scheduling environment (7-min slots)\n",
        "import os\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "log_dir = \"/content/logs\" if os.path.exists(\"/content\") else \"/workspace/logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# make_vec_env handles Monitor and Gymnasium compatibility wrappers\n",
        "vec_env = make_vec_env(lambda: ClinicSchedulingEnv(slot_minutes=7), n_envs=1, monitor_dir=log_dir)\n",
        "\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    vec_env,\n",
        "    verbose=1,\n",
        "    learning_rate=3e-4,\n",
        "    n_steps=2048,\n",
        "    batch_size=256,\n",
        "    n_epochs=10,\n",
        "    gamma=0.995,\n",
        "    gae_lambda=0.95,\n",
        ")\n",
        "\n",
        "# Train\n",
        "timesteps = 200_000\n",
        "model.learn(total_timesteps=timesteps)\n",
        "\n",
        "# Save\n",
        "model_path = os.path.join(log_dir, \"ppo_clinic_scheduling\")\n",
        "model.save(model_path)\n",
        "print(\"Saved model to\", model_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | 49.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 899      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 116         |\n",
            "|    ep_rew_mean          | 49.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 843         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013261056 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.045379877 |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.71        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00547    |\n",
            "|    value_loss           | 27.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 108         |\n",
            "|    ep_rew_mean          | 51.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 864         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012949418 |\n",
            "|    clip_fraction        | 0.0698      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0.2005496   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.76        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00533    |\n",
            "|    value_loss           | 29.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 109         |\n",
            "|    ep_rew_mean          | 51.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 871         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007314346 |\n",
            "|    clip_fraction        | 0.0083      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.37219578  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.3        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00376    |\n",
            "|    value_loss           | 46.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 105         |\n",
            "|    ep_rew_mean          | 52.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 876         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011075012 |\n",
            "|    clip_fraction        | 0.0208      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.97       |\n",
            "|    explained_variance   | 0.4728608   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.3        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00317    |\n",
            "|    value_loss           | 31          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 103         |\n",
            "|    ep_rew_mean          | 53          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 870         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005365766 |\n",
            "|    clip_fraction        | 0.0101      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.981      |\n",
            "|    explained_variance   | 0.55037653  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 17.6        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.00183    |\n",
            "|    value_loss           | 46.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 101         |\n",
            "|    ep_rew_mean          | 53.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 855         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008977294 |\n",
            "|    clip_fraction        | 0.0529      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.59634614  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.1        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00416    |\n",
            "|    value_loss           | 33.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 102         |\n",
            "|    ep_rew_mean          | 53          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 863         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007908056 |\n",
            "|    clip_fraction        | 0.0269      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.70700455  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.4        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.00538    |\n",
            "|    value_loss           | 25.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 101         |\n",
            "|    ep_rew_mean          | 53.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 865         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017008483 |\n",
            "|    clip_fraction        | 0.0667      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.7411234   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.3        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00515    |\n",
            "|    value_loss           | 30.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 106        |\n",
            "|    ep_rew_mean          | 52.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 870        |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 23         |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01417998 |\n",
            "|    clip_fraction        | 0.157      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.03      |\n",
            "|    explained_variance   | 0.8012514  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 9.07       |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | -0.0062    |\n",
            "|    value_loss           | 21.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 107         |\n",
            "|    ep_rew_mean          | 51.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 874         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012210087 |\n",
            "|    clip_fraction        | 0.0852      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.7980175   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.46        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00567    |\n",
            "|    value_loss           | 18.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 107         |\n",
            "|    ep_rew_mean          | 51.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 859         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 28          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009264611 |\n",
            "|    clip_fraction        | 0.0405      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.86522514  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.56        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00565    |\n",
            "|    value_loss           | 18.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 106         |\n",
            "|    ep_rew_mean          | 52.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 863         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 30          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009910973 |\n",
            "|    clip_fraction        | 0.0356      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.989      |\n",
            "|    explained_variance   | 0.8855445   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.06        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00326    |\n",
            "|    value_loss           | 12.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 107         |\n",
            "|    ep_rew_mean          | 51.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 864         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 33          |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014257404 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.977      |\n",
            "|    explained_variance   | 0.8998704   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.04        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0055     |\n",
            "|    value_loss           | 14.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 107         |\n",
            "|    ep_rew_mean          | 51.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 865         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 35          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011070421 |\n",
            "|    clip_fraction        | 0.0889      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.966      |\n",
            "|    explained_variance   | 0.9095216   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.06        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.00532    |\n",
            "|    value_loss           | 11          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 107         |\n",
            "|    ep_rew_mean          | 51.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 869         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 37          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008645062 |\n",
            "|    clip_fraction        | 0.0307      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.975      |\n",
            "|    explained_variance   | 0.9418737   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.83        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0037     |\n",
            "|    value_loss           | 7.37        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 106         |\n",
            "|    ep_rew_mean          | 52.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 862         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 40          |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016338274 |\n",
            "|    clip_fraction        | 0.149       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.935      |\n",
            "|    explained_variance   | 0.9550975   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.55        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.00905    |\n",
            "|    value_loss           | 6.14        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 106         |\n",
            "|    ep_rew_mean          | 52.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 863         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 42          |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013917658 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.848      |\n",
            "|    explained_variance   | 0.95630807  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.83        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.00941    |\n",
            "|    value_loss           | 5.87        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 106          |\n",
            "|    ep_rew_mean          | 52.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 865          |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 38912        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058246446 |\n",
            "|    clip_fraction        | 0.0305       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.803       |\n",
            "|    explained_variance   | 0.9535287    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.64         |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00469     |\n",
            "|    value_loss           | 7.06         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 102         |\n",
            "|    ep_rew_mean          | 53.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 868         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 47          |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009307174 |\n",
            "|    clip_fraction        | 0.0536      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.732      |\n",
            "|    explained_variance   | 0.9642609   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.08        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00684    |\n",
            "|    value_loss           | 5.42        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 97.2        |\n",
            "|    ep_rew_mean          | 54.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 871         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004156676 |\n",
            "|    clip_fraction        | 0.0392      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.752      |\n",
            "|    explained_variance   | 0.96229225  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.48        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.00435    |\n",
            "|    value_loss           | 6.01        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 93.7        |\n",
            "|    ep_rew_mean          | 55.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 862         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 52          |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004982059 |\n",
            "|    clip_fraction        | 0.0257      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.704      |\n",
            "|    explained_variance   | 0.9393113   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.1         |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00415    |\n",
            "|    value_loss           | 9.4         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 93           |\n",
            "|    ep_rew_mean          | 55.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 860          |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 54           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065497654 |\n",
            "|    clip_fraction        | 0.0369       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.741       |\n",
            "|    explained_variance   | 0.96033686   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.13         |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.00369     |\n",
            "|    value_loss           | 6.45         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 93.4        |\n",
            "|    ep_rew_mean          | 55.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 862         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008008296 |\n",
            "|    clip_fraction        | 0.0618      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.708      |\n",
            "|    explained_variance   | 0.9676185   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.55        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.00504    |\n",
            "|    value_loss           | 5.15        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 95.1         |\n",
            "|    ep_rew_mean          | 54.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 864          |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 59           |\n",
            "|    total_timesteps      | 51200        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045217494 |\n",
            "|    clip_fraction        | 0.0276       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.65        |\n",
            "|    explained_variance   | 0.9703504    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.08         |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.00402     |\n",
            "|    value_loss           | 4.52         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 95.2         |\n",
            "|    ep_rew_mean          | 54.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 866          |\n",
            "|    iterations           | 26           |\n",
            "|    time_elapsed         | 61           |\n",
            "|    total_timesteps      | 53248        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015045574 |\n",
            "|    clip_fraction        | 0.00737      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.624       |\n",
            "|    explained_variance   | 0.96945614   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.21         |\n",
            "|    n_updates            | 250          |\n",
            "|    policy_gradient_loss | -0.00166     |\n",
            "|    value_loss           | 4.87         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 94.8        |\n",
            "|    ep_rew_mean          | 54.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 832         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 66          |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003981839 |\n",
            "|    clip_fraction        | 0.0268      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.648      |\n",
            "|    explained_variance   | 0.97064316  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.72        |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00236    |\n",
            "|    value_loss           | 4.55        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 93.4         |\n",
            "|    ep_rew_mean          | 55.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 816          |\n",
            "|    iterations           | 28           |\n",
            "|    time_elapsed         | 70           |\n",
            "|    total_timesteps      | 57344        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041391193 |\n",
            "|    clip_fraction        | 0.0239       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.634       |\n",
            "|    explained_variance   | 0.9684482    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.49         |\n",
            "|    n_updates            | 270          |\n",
            "|    policy_gradient_loss | -0.00303     |\n",
            "|    value_loss           | 4.77         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 92.6         |\n",
            "|    ep_rew_mean          | 55.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 819          |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 72           |\n",
            "|    total_timesteps      | 59392        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027909416 |\n",
            "|    clip_fraction        | 0.00942      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.607       |\n",
            "|    explained_variance   | 0.97253      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.49         |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -0.00207     |\n",
            "|    value_loss           | 4.47         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 92.8         |\n",
            "|    ep_rew_mean          | 55.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 823          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 74           |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040852213 |\n",
            "|    clip_fraction        | 0.0297       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.575       |\n",
            "|    explained_variance   | 0.9767771    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.89         |\n",
            "|    n_updates            | 290          |\n",
            "|    policy_gradient_loss | -0.00394     |\n",
            "|    value_loss           | 3.74         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 90.2        |\n",
            "|    ep_rew_mean          | 56          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 824         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 77          |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001618607 |\n",
            "|    clip_fraction        | 0.018       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.549      |\n",
            "|    explained_variance   | 0.98006624  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.56        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.00252    |\n",
            "|    value_loss           | 3.4         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 90.1         |\n",
            "|    ep_rew_mean          | 56           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 818          |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 80           |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016867518 |\n",
            "|    clip_fraction        | 0.0121       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.564       |\n",
            "|    explained_variance   | 0.9694147    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.18         |\n",
            "|    n_updates            | 310          |\n",
            "|    policy_gradient_loss | -0.00194     |\n",
            "|    value_loss           | 5.84         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 88          |\n",
            "|    ep_rew_mean          | 56.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 813         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001446832 |\n",
            "|    clip_fraction        | 0.0271      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.594      |\n",
            "|    explained_variance   | 0.9774661   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2           |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.00219    |\n",
            "|    value_loss           | 4.05        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 88           |\n",
            "|    ep_rew_mean          | 56.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 816          |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 85           |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022812346 |\n",
            "|    clip_fraction        | 0.0124       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.568       |\n",
            "|    explained_variance   | 0.9757097    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.13         |\n",
            "|    n_updates            | 330          |\n",
            "|    policy_gradient_loss | -0.00174     |\n",
            "|    value_loss           | 4.38         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 87.9         |\n",
            "|    ep_rew_mean          | 56.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 819          |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 87           |\n",
            "|    total_timesteps      | 71680        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033938328 |\n",
            "|    clip_fraction        | 0.0362       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.543       |\n",
            "|    explained_variance   | 0.98211515   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.42         |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.00353     |\n",
            "|    value_loss           | 2.98         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 89.9        |\n",
            "|    ep_rew_mean          | 56.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 819         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 89          |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003256235 |\n",
            "|    clip_fraction        | 0.0447      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.537      |\n",
            "|    explained_variance   | 0.97545654  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.84        |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.00649    |\n",
            "|    value_loss           | 4.07        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 90.6        |\n",
            "|    ep_rew_mean          | 56          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 817         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 92          |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001693377 |\n",
            "|    clip_fraction        | 0.0224      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.52       |\n",
            "|    explained_variance   | 0.9816545   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.46        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.00216    |\n",
            "|    value_loss           | 3.22        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 89.7        |\n",
            "|    ep_rew_mean          | 56.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 819         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 95          |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002924147 |\n",
            "|    clip_fraction        | 0.0322      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.575      |\n",
            "|    explained_variance   | 0.9804412   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.65        |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.00326    |\n",
            "|    value_loss           | 2.98        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 88.9         |\n",
            "|    ep_rew_mean          | 56.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 821          |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 97           |\n",
            "|    total_timesteps      | 79872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034968231 |\n",
            "|    clip_fraction        | 0.0299       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.565       |\n",
            "|    explained_variance   | 0.9813688    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.17         |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.0044      |\n",
            "|    value_loss           | 3.22         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 89.7        |\n",
            "|    ep_rew_mean          | 56.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 823         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 99          |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004108759 |\n",
            "|    clip_fraction        | 0.017       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.561      |\n",
            "|    explained_variance   | 0.97830087  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.57        |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.00216    |\n",
            "|    value_loss           | 4.14        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 88.9         |\n",
            "|    ep_rew_mean          | 56.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 825          |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 101          |\n",
            "|    total_timesteps      | 83968        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036232104 |\n",
            "|    clip_fraction        | 0.0264       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.57        |\n",
            "|    explained_variance   | 0.98190075   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.54         |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | -0.00489     |\n",
            "|    value_loss           | 3.03         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 89           |\n",
            "|    ep_rew_mean          | 56.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 822          |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 104          |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019072056 |\n",
            "|    clip_fraction        | 0.00479      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.57        |\n",
            "|    explained_variance   | 0.97679573   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.76         |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | -0.00191     |\n",
            "|    value_loss           | 3.83         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 91.7        |\n",
            "|    ep_rew_mean          | 55.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 824         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 106         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004628988 |\n",
            "|    clip_fraction        | 0.0388      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.531      |\n",
            "|    explained_variance   | 0.9855476   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.997       |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.00263    |\n",
            "|    value_loss           | 2.24        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 92.9         |\n",
            "|    ep_rew_mean          | 55.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 826          |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 108          |\n",
            "|    total_timesteps      | 90112        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014400848 |\n",
            "|    clip_fraction        | 0.00337      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.478       |\n",
            "|    explained_variance   | 0.98371524   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.1          |\n",
            "|    n_updates            | 430          |\n",
            "|    policy_gradient_loss | -0.00129     |\n",
            "|    value_loss           | 2.72         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 91           |\n",
            "|    ep_rew_mean          | 55.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 828          |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 111          |\n",
            "|    total_timesteps      | 92160        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013217926 |\n",
            "|    clip_fraction        | 0.0108       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.514       |\n",
            "|    explained_variance   | 0.9793593    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.91         |\n",
            "|    n_updates            | 440          |\n",
            "|    policy_gradient_loss | -0.00127     |\n",
            "|    value_loss           | 3.4          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 91.8        |\n",
            "|    ep_rew_mean          | 55.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 830         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 113         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004970636 |\n",
            "|    clip_fraction        | 0.041       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.477      |\n",
            "|    explained_variance   | 0.9844874   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.39        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00475    |\n",
            "|    value_loss           | 2.7         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 88.1         |\n",
            "|    ep_rew_mean          | 56.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 829          |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 116          |\n",
            "|    total_timesteps      | 96256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016749848 |\n",
            "|    clip_fraction        | 0.0127       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.458       |\n",
            "|    explained_variance   | 0.98153055   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.51         |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -0.00177     |\n",
            "|    value_loss           | 2.98         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 87.4         |\n",
            "|    ep_rew_mean          | 56.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 829          |\n",
            "|    iterations           | 48           |\n",
            "|    time_elapsed         | 118          |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018965943 |\n",
            "|    clip_fraction        | 0.00791      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.463       |\n",
            "|    explained_variance   | 0.98004866   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.64         |\n",
            "|    n_updates            | 470          |\n",
            "|    policy_gradient_loss | -0.00227     |\n",
            "|    value_loss           | 3.8          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.8         |\n",
            "|    ep_rew_mean          | 56.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 830          |\n",
            "|    iterations           | 49           |\n",
            "|    time_elapsed         | 120          |\n",
            "|    total_timesteps      | 100352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049089715 |\n",
            "|    clip_fraction        | 0.0492       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.482       |\n",
            "|    explained_variance   | 0.978555     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.85         |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -0.00498     |\n",
            "|    value_loss           | 3.59         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 85.8       |\n",
            "|    ep_rew_mean          | 57.1       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 832        |\n",
            "|    iterations           | 50         |\n",
            "|    time_elapsed         | 122        |\n",
            "|    total_timesteps      | 102400     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00379316 |\n",
            "|    clip_fraction        | 0.0292     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.472     |\n",
            "|    explained_variance   | 0.9823596  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.37       |\n",
            "|    n_updates            | 490        |\n",
            "|    policy_gradient_loss | -0.00284   |\n",
            "|    value_loss           | 3.44       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 87.3        |\n",
            "|    ep_rew_mean          | 56.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 833         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 125         |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002996349 |\n",
            "|    clip_fraction        | 0.0193      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.48       |\n",
            "|    explained_variance   | 0.9823177   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.53        |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.0034     |\n",
            "|    value_loss           | 3.02        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 87.7        |\n",
            "|    ep_rew_mean          | 56.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 832         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 127         |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002261967 |\n",
            "|    clip_fraction        | 0.0152      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.473      |\n",
            "|    explained_variance   | 0.9855086   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.26        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.00206    |\n",
            "|    value_loss           | 2.63        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 89           |\n",
            "|    ep_rew_mean          | 56.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 830          |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 130          |\n",
            "|    total_timesteps      | 108544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014437635 |\n",
            "|    clip_fraction        | 0.00229      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.489       |\n",
            "|    explained_variance   | 0.9821141    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.04         |\n",
            "|    n_updates            | 520          |\n",
            "|    policy_gradient_loss | -0.00179     |\n",
            "|    value_loss           | 3.25         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.3         |\n",
            "|    ep_rew_mean          | 56.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 831          |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 133          |\n",
            "|    total_timesteps      | 110592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050528543 |\n",
            "|    clip_fraction        | 0.0587       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.474       |\n",
            "|    explained_variance   | 0.9805225    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.64         |\n",
            "|    n_updates            | 530          |\n",
            "|    policy_gradient_loss | -0.00408     |\n",
            "|    value_loss           | 3.11         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.1         |\n",
            "|    ep_rew_mean          | 56.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 832          |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 135          |\n",
            "|    total_timesteps      | 112640       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015400096 |\n",
            "|    clip_fraction        | 0.0104       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.485       |\n",
            "|    explained_variance   | 0.98203284   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.49         |\n",
            "|    n_updates            | 540          |\n",
            "|    policy_gradient_loss | -0.00268     |\n",
            "|    value_loss           | 3.43         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 84.3          |\n",
            "|    ep_rew_mean          | 57.3          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 833           |\n",
            "|    iterations           | 56            |\n",
            "|    time_elapsed         | 137           |\n",
            "|    total_timesteps      | 114688        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00092614617 |\n",
            "|    clip_fraction        | 0.0061        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.478        |\n",
            "|    explained_variance   | 0.983606      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.1           |\n",
            "|    n_updates            | 550           |\n",
            "|    policy_gradient_loss | -0.00201      |\n",
            "|    value_loss           | 3.03          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 83.9         |\n",
            "|    ep_rew_mean          | 57.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 832          |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 140          |\n",
            "|    total_timesteps      | 116736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016210818 |\n",
            "|    clip_fraction        | 0.00576      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.459       |\n",
            "|    explained_variance   | 0.9823551    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.57         |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | -0.00137     |\n",
            "|    value_loss           | 3.37         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 87          |\n",
            "|    ep_rew_mean          | 56.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 828         |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 143         |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003733802 |\n",
            "|    clip_fraction        | 0.013       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.452      |\n",
            "|    explained_variance   | 0.98627865  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.2         |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.00234    |\n",
            "|    value_loss           | 2.6         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86           |\n",
            "|    ep_rew_mean          | 56.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 828          |\n",
            "|    iterations           | 59           |\n",
            "|    time_elapsed         | 145          |\n",
            "|    total_timesteps      | 120832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025233023 |\n",
            "|    clip_fraction        | 0.0208       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.454       |\n",
            "|    explained_variance   | 0.98395437   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.53         |\n",
            "|    n_updates            | 580          |\n",
            "|    policy_gradient_loss | -0.00386     |\n",
            "|    value_loss           | 2.84         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 88.5        |\n",
            "|    ep_rew_mean          | 56.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 829         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 148         |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002221378 |\n",
            "|    clip_fraction        | 0.0127      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.445      |\n",
            "|    explained_variance   | 0.9810362   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.87        |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | -0.00337    |\n",
            "|    value_loss           | 3.45        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 89.5         |\n",
            "|    ep_rew_mean          | 56.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 830          |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 150          |\n",
            "|    total_timesteps      | 124928       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010436613 |\n",
            "|    clip_fraction        | 0.00464      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.433       |\n",
            "|    explained_variance   | 0.9858507    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.13         |\n",
            "|    n_updates            | 600          |\n",
            "|    policy_gradient_loss | -0.00134     |\n",
            "|    value_loss           | 2.48         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 87.6         |\n",
            "|    ep_rew_mean          | 56.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 829          |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 153          |\n",
            "|    total_timesteps      | 126976       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022996224 |\n",
            "|    clip_fraction        | 0.019        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.398       |\n",
            "|    explained_variance   | 0.9847954    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.49         |\n",
            "|    n_updates            | 610          |\n",
            "|    policy_gradient_loss | -0.00202     |\n",
            "|    value_loss           | 2.72         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 88.3         |\n",
            "|    ep_rew_mean          | 56.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 828          |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 155          |\n",
            "|    total_timesteps      | 129024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027377668 |\n",
            "|    clip_fraction        | 0.0115       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.404       |\n",
            "|    explained_variance   | 0.98205703   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.69         |\n",
            "|    n_updates            | 620          |\n",
            "|    policy_gradient_loss | -0.00149     |\n",
            "|    value_loss           | 3.32         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 89.9         |\n",
            "|    ep_rew_mean          | 56.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 829          |\n",
            "|    iterations           | 64           |\n",
            "|    time_elapsed         | 157          |\n",
            "|    total_timesteps      | 131072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028203975 |\n",
            "|    clip_fraction        | 0.0187       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.344       |\n",
            "|    explained_variance   | 0.98249084   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.42         |\n",
            "|    n_updates            | 630          |\n",
            "|    policy_gradient_loss | -0.00158     |\n",
            "|    value_loss           | 3.03         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 89           |\n",
            "|    ep_rew_mean          | 56.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 831          |\n",
            "|    iterations           | 65           |\n",
            "|    time_elapsed         | 160          |\n",
            "|    total_timesteps      | 133120       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009314419 |\n",
            "|    clip_fraction        | 0.00742      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.338       |\n",
            "|    explained_variance   | 0.98253995   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.3          |\n",
            "|    n_updates            | 640          |\n",
            "|    policy_gradient_loss | -0.00117     |\n",
            "|    value_loss           | 2.74         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 88.7        |\n",
            "|    ep_rew_mean          | 56.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 832         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 162         |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002486073 |\n",
            "|    clip_fraction        | 0.0231      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.335      |\n",
            "|    explained_variance   | 0.98381835  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.991       |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | -0.00214    |\n",
            "|    value_loss           | 2.54        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 88.1        |\n",
            "|    ep_rew_mean          | 56.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 832         |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 164         |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000565534 |\n",
            "|    clip_fraction        | 0.00259     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.351      |\n",
            "|    explained_variance   | 0.98612803  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.71        |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | -0.000432   |\n",
            "|    value_loss           | 2.55        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 88.6         |\n",
            "|    ep_rew_mean          | 56.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 831          |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 167          |\n",
            "|    total_timesteps      | 139264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012979782 |\n",
            "|    clip_fraction        | 0.0132       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.369       |\n",
            "|    explained_variance   | 0.9848758    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.05         |\n",
            "|    n_updates            | 670          |\n",
            "|    policy_gradient_loss | -0.00284     |\n",
            "|    value_loss           | 2.63         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 87.3         |\n",
            "|    ep_rew_mean          | 56.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 832          |\n",
            "|    iterations           | 69           |\n",
            "|    time_elapsed         | 169          |\n",
            "|    total_timesteps      | 141312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017213798 |\n",
            "|    clip_fraction        | 0.0197       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.375       |\n",
            "|    explained_variance   | 0.9834805    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.21         |\n",
            "|    n_updates            | 680          |\n",
            "|    policy_gradient_loss | -0.00342     |\n",
            "|    value_loss           | 2.98         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 85.9        |\n",
            "|    ep_rew_mean          | 57          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 833         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 171         |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004884172 |\n",
            "|    clip_fraction        | 0.045       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.373      |\n",
            "|    explained_variance   | 0.9850803   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.04        |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.00354    |\n",
            "|    value_loss           | 2.49        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 85.9         |\n",
            "|    ep_rew_mean          | 56.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 834          |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 174          |\n",
            "|    total_timesteps      | 145408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017686987 |\n",
            "|    clip_fraction        | 0.0117       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.395       |\n",
            "|    explained_variance   | 0.9860373    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.16         |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -0.00159     |\n",
            "|    value_loss           | 2.57         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.2         |\n",
            "|    ep_rew_mean          | 56.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 835          |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 176          |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030306142 |\n",
            "|    clip_fraction        | 0.00952      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.365       |\n",
            "|    explained_variance   | 0.98568285   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.5          |\n",
            "|    n_updates            | 710          |\n",
            "|    policy_gradient_loss | -0.0015      |\n",
            "|    value_loss           | 2.69         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 85.2        |\n",
            "|    ep_rew_mean          | 57          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 833         |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 179         |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000984466 |\n",
            "|    clip_fraction        | 0.00508     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.323      |\n",
            "|    explained_variance   | 0.98377377  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.41        |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.00151    |\n",
            "|    value_loss           | 2.98        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 85.2        |\n",
            "|    ep_rew_mean          | 57.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 834         |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 181         |\n",
            "|    total_timesteps      | 151552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004002604 |\n",
            "|    clip_fraction        | 0.0406      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.372      |\n",
            "|    explained_variance   | 0.98596394  |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.19        |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | -0.00375    |\n",
            "|    value_loss           | 2.44        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.2         |\n",
            "|    ep_rew_mean          | 56.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 835          |\n",
            "|    iterations           | 75           |\n",
            "|    time_elapsed         | 183          |\n",
            "|    total_timesteps      | 153600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013612281 |\n",
            "|    clip_fraction        | 0.0136       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.37        |\n",
            "|    explained_variance   | 0.9825497    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.58         |\n",
            "|    n_updates            | 740          |\n",
            "|    policy_gradient_loss | -0.00284     |\n",
            "|    value_loss           | 3.01         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 85           |\n",
            "|    ep_rew_mean          | 57.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 836          |\n",
            "|    iterations           | 76           |\n",
            "|    time_elapsed         | 186          |\n",
            "|    total_timesteps      | 155648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017958577 |\n",
            "|    clip_fraction        | 0.00962      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.398       |\n",
            "|    explained_variance   | 0.98495996   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.28         |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    value_loss           | 2.8          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 85.1         |\n",
            "|    ep_rew_mean          | 57           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 837          |\n",
            "|    iterations           | 77           |\n",
            "|    time_elapsed         | 188          |\n",
            "|    total_timesteps      | 157696       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025559412 |\n",
            "|    clip_fraction        | 0.0391       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.432       |\n",
            "|    explained_variance   | 0.98536414   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.39         |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.00104     |\n",
            "|    value_loss           | 2.79         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 84           |\n",
            "|    ep_rew_mean          | 57.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 837          |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 190          |\n",
            "|    total_timesteps      | 159744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0083465595 |\n",
            "|    clip_fraction        | 0.0431       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.388       |\n",
            "|    explained_variance   | 0.9847952    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.12         |\n",
            "|    n_updates            | 770          |\n",
            "|    policy_gradient_loss | -0.00374     |\n",
            "|    value_loss           | 2.7          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 84           |\n",
            "|    ep_rew_mean          | 57.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 836          |\n",
            "|    iterations           | 79           |\n",
            "|    time_elapsed         | 193          |\n",
            "|    total_timesteps      | 161792       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022398084 |\n",
            "|    clip_fraction        | 0.0224       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.378       |\n",
            "|    explained_variance   | 0.98227185   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.47         |\n",
            "|    n_updates            | 780          |\n",
            "|    policy_gradient_loss | -0.00476     |\n",
            "|    value_loss           | 3.02         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 84.2         |\n",
            "|    ep_rew_mean          | 57.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 837          |\n",
            "|    iterations           | 80           |\n",
            "|    time_elapsed         | 195          |\n",
            "|    total_timesteps      | 163840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028303042 |\n",
            "|    clip_fraction        | 0.00942      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.393       |\n",
            "|    explained_variance   | 0.9834739    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.37         |\n",
            "|    n_updates            | 790          |\n",
            "|    policy_gradient_loss | -0.00105     |\n",
            "|    value_loss           | 2.94         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 82.7         |\n",
            "|    ep_rew_mean          | 57.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 838          |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 197          |\n",
            "|    total_timesteps      | 165888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026062867 |\n",
            "|    clip_fraction        | 0.0173       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.334       |\n",
            "|    explained_variance   | 0.98653436   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.36         |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | -0.00272     |\n",
            "|    value_loss           | 2.43         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 81.9       |\n",
            "|    ep_rew_mean          | 57.7       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 838        |\n",
            "|    iterations           | 82         |\n",
            "|    time_elapsed         | 200        |\n",
            "|    total_timesteps      | 167936     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00206434 |\n",
            "|    clip_fraction        | 0.0162     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.358     |\n",
            "|    explained_variance   | 0.98191214 |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.29       |\n",
            "|    n_updates            | 810        |\n",
            "|    policy_gradient_loss | -0.00196   |\n",
            "|    value_loss           | 2.93       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 84.7         |\n",
            "|    ep_rew_mean          | 57.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 838          |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 202          |\n",
            "|    total_timesteps      | 169984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018706556 |\n",
            "|    clip_fraction        | 0.01         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.348       |\n",
            "|    explained_variance   | 0.9876273    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.18         |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | -0.00077     |\n",
            "|    value_loss           | 2.41         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 84.2         |\n",
            "|    ep_rew_mean          | 57.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 837          |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 205          |\n",
            "|    total_timesteps      | 172032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015255162 |\n",
            "|    clip_fraction        | 0.0101       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.366       |\n",
            "|    explained_variance   | 0.98344547   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.43         |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | -0.0027      |\n",
            "|    value_loss           | 2.89         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 84.7         |\n",
            "|    ep_rew_mean          | 57.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 838          |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 207          |\n",
            "|    total_timesteps      | 174080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023423661 |\n",
            "|    clip_fraction        | 0.0191       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.374       |\n",
            "|    explained_variance   | 0.9853033    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.25         |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | -0.00176     |\n",
            "|    value_loss           | 2.73         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86           |\n",
            "|    ep_rew_mean          | 57           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 838          |\n",
            "|    iterations           | 86           |\n",
            "|    time_elapsed         | 209          |\n",
            "|    total_timesteps      | 176128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016366147 |\n",
            "|    clip_fraction        | 0.00859      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.393       |\n",
            "|    explained_variance   | 0.9836231    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.79         |\n",
            "|    n_updates            | 850          |\n",
            "|    policy_gradient_loss | -0.0027      |\n",
            "|    value_loss           | 2.99         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 83.6        |\n",
            "|    ep_rew_mean          | 57.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 839         |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 212         |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003702557 |\n",
            "|    clip_fraction        | 0.0144      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.363      |\n",
            "|    explained_variance   | 0.9837583   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.52        |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | -0.00215    |\n",
            "|    value_loss           | 3.06        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 83.1         |\n",
            "|    ep_rew_mean          | 57.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 839          |\n",
            "|    iterations           | 88           |\n",
            "|    time_elapsed         | 214          |\n",
            "|    total_timesteps      | 180224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046577584 |\n",
            "|    clip_fraction        | 0.0271       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.362       |\n",
            "|    explained_variance   | 0.98505163   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.37         |\n",
            "|    n_updates            | 870          |\n",
            "|    policy_gradient_loss | -0.00191     |\n",
            "|    value_loss           | 2.95         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 85.4        |\n",
            "|    ep_rew_mean          | 56.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 838         |\n",
            "|    iterations           | 89          |\n",
            "|    time_elapsed         | 217         |\n",
            "|    total_timesteps      | 182272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004236011 |\n",
            "|    clip_fraction        | 0.0377      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.342      |\n",
            "|    explained_variance   | 0.980958    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.08        |\n",
            "|    n_updates            | 880         |\n",
            "|    policy_gradient_loss | -0.00449    |\n",
            "|    value_loss           | 3.33        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.8         |\n",
            "|    ep_rew_mean          | 56.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 839          |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 219          |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011645674 |\n",
            "|    clip_fraction        | 0.00986      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.304       |\n",
            "|    explained_variance   | 0.9849136    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.23         |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | -0.000665    |\n",
            "|    value_loss           | 2.51         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.4         |\n",
            "|    ep_rew_mean          | 56.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 839          |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 221          |\n",
            "|    total_timesteps      | 186368       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019324799 |\n",
            "|    clip_fraction        | 0.022        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.284       |\n",
            "|    explained_variance   | 0.98599064   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.35         |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | -0.00299     |\n",
            "|    value_loss           | 2.6          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 85.6         |\n",
            "|    ep_rew_mean          | 56.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 840          |\n",
            "|    iterations           | 92           |\n",
            "|    time_elapsed         | 224          |\n",
            "|    total_timesteps      | 188416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014609287 |\n",
            "|    clip_fraction        | 0.00703      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.32        |\n",
            "|    explained_variance   | 0.98252356   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.95         |\n",
            "|    n_updates            | 910          |\n",
            "|    policy_gradient_loss | -0.00123     |\n",
            "|    value_loss           | 3.09         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 82.6          |\n",
            "|    ep_rew_mean          | 57.6          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 841           |\n",
            "|    iterations           | 93            |\n",
            "|    time_elapsed         | 226           |\n",
            "|    total_timesteps      | 190464        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00046642093 |\n",
            "|    clip_fraction        | 0.0102        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.335        |\n",
            "|    explained_variance   | 0.98274785    |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.45          |\n",
            "|    n_updates            | 920           |\n",
            "|    policy_gradient_loss | -0.00113      |\n",
            "|    value_loss           | 3.17          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 82.2        |\n",
            "|    ep_rew_mean          | 57.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 839         |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 229         |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001088246 |\n",
            "|    clip_fraction        | 0.0119      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.309      |\n",
            "|    explained_variance   | 0.984673    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.49        |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | -0.000813   |\n",
            "|    value_loss           | 2.92        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 83.6         |\n",
            "|    ep_rew_mean          | 57.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 840          |\n",
            "|    iterations           | 95           |\n",
            "|    time_elapsed         | 231          |\n",
            "|    total_timesteps      | 194560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010981609 |\n",
            "|    clip_fraction        | 0.00601      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.266       |\n",
            "|    explained_variance   | 0.98512256   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.05         |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.00247     |\n",
            "|    value_loss           | 2.17         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.6         |\n",
            "|    ep_rew_mean          | 56.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 840          |\n",
            "|    iterations           | 96           |\n",
            "|    time_elapsed         | 233          |\n",
            "|    total_timesteps      | 196608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011000859 |\n",
            "|    clip_fraction        | 0.0112       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.259       |\n",
            "|    explained_variance   | 0.9874162    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.678        |\n",
            "|    n_updates            | 950          |\n",
            "|    policy_gradient_loss | -0.00256     |\n",
            "|    value_loss           | 2.05         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 87.4         |\n",
            "|    ep_rew_mean          | 56.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 841          |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 236          |\n",
            "|    total_timesteps      | 198656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022071404 |\n",
            "|    clip_fraction        | 0.00981      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.263       |\n",
            "|    explained_variance   | 0.98800814   |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01         |\n",
            "|    n_updates            | 960          |\n",
            "|    policy_gradient_loss | -0.00146     |\n",
            "|    value_loss           | 2.16         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 86.2          |\n",
            "|    ep_rew_mean          | 56.7          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 841           |\n",
            "|    iterations           | 98            |\n",
            "|    time_elapsed         | 238           |\n",
            "|    total_timesteps      | 200704        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00036032152 |\n",
            "|    clip_fraction        | 0.0166        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.264        |\n",
            "|    explained_variance   | 0.9865553     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.39          |\n",
            "|    n_updates            | 970           |\n",
            "|    policy_gradient_loss | -0.000694     |\n",
            "|    value_loss           | 2.51          |\n",
            "-------------------------------------------\n",
            "Saved model to /content/logs/ppo_clinic_scheduling\n"
          ]
        }
      ],
      "id": "de8ad6a1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc5b0f64",
        "outputId": "f96d8a39-a75f-41f8-8a70-a4302eec5006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Evaluation and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# load\n",
        "loaded = PPO.load(model_path)\n",
        "\n",
        "def run_episode(env_seed=None):\n",
        "    env = ClinicSchedulingEnv(slot_minutes=10, seed=env_seed)\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    events = []\n",
        "    t = 0\n",
        "    while not done:\n",
        "        action, _ = loaded.predict(obs, deterministic=True)\n",
        "        prev_min = env.minute\n",
        "        prev_walk = len(env.walkin_queue)\n",
        "        prev_late = len(env.late_list)\n",
        "        obs, reward, terminated, truncated, info = env.step(int(action))\n",
        "        done = terminated or truncated\n",
        "        events.append({\n",
        "            \"t\": t,\n",
        "            \"minute\": prev_min,\n",
        "            \"action\": int(action),\n",
        "            \"reward\": float(reward),\n",
        "            \"served\": len(env.served_ids),\n",
        "            \"walkin_q\": prev_walk,\n",
        "            \"late\": prev_late,\n",
        "        })\n",
        "        t += 1\n",
        "    return pd.DataFrame(events)\n",
        "\n",
        "df = run_episode(env_seed=123)\n",
        "fig = px.line(df, x=\"minute\", y=[\"served\", \"walkin_q\", \"late\"], title=\"Clinic metrics over time\")\n",
        "fig.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 35)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    df = run_episode(env_seed=123)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "id": "dc5b0f64"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c38302f"
      },
      "source": [
        "# Improved evaluation: metrics and charts (7-min slots)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Load trained model\n",
        "loaded = PPO.load(model_path)\n",
        "\n",
        "\n",
        "def run_episode(env_seed=None, walkin_cutoff_minute=None):\n",
        "    env = ClinicSchedulingEnv(slot_minutes=7, seed=env_seed, walkin_cutoff_minute=walkin_cutoff_minute)\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    timeline = []\n",
        "    step_idx = 0\n",
        "    while not done:\n",
        "        action, _ = loaded.predict(obs, deterministic=True)\n",
        "        prev_state = {\n",
        "            \"step\": step_idx,\n",
        "            \"minute\": env.minute,\n",
        "            \"walkin_q\": len(env.walkin_queue),\n",
        "            \"late\": len(env.late_list),\n",
        "            \"served\": len(env.served_ids),\n",
        "            \"action\": int(action),\n",
        "        }\n",
        "        obs, reward, terminated, truncated, _ = env.step(int(action))\n",
        "        done = bool(terminated or truncated)\n",
        "        prev_state[\"reward\"] = float(reward)\n",
        "        timeline.append(prev_state)\n",
        "        step_idx += 1\n",
        "    # Build DataFrames\n",
        "    timeline_df = pd.DataFrame(timeline)\n",
        "    served_df = pd.DataFrame(env.served_log)\n",
        "    return env, timeline_df, served_df\n",
        "\n",
        "\n",
        "# Run one evaluation episode\n",
        "env, timeline_df, served_df = run_episode(env_seed=123)\n",
        "\n",
        "# Summary metrics\n",
        "scheduled_total = len(env.scheduled)\n",
        "served_total = len(served_df)\n",
        "served_scheduled = int((~served_df[\"is_walkin\"]).sum()) if not served_df.empty else 0\n",
        "served_walkin = int((served_df[\"is_walkin\"]).sum()) if not served_df.empty else 0\n",
        "served_via_recall = int((served_df[\"served_via_recall\"]).sum()) if not served_df.empty else 0\n",
        "avg_wait_scheduled = float(served_df.loc[~served_df[\"is_walkin\"], \"wait_minutes\"].dropna().mean()) if served_scheduled > 0 else np.nan\n",
        "avg_wait_walkin = float(served_df.loc[served_df[\"is_walkin\"], \"wait_minutes\"].dropna().mean()) if served_walkin > 0 else np.nan\n",
        "\n",
        "print(\"Scheduled total:\", scheduled_total)\n",
        "print(\"Served total:\", served_total, \"(scheduled:\", served_scheduled, \", walk-in:\", served_walkin, \")\")\n",
        "print(\"Late recalls served:\", served_via_recall)\n",
        "print(\"Avg wait (scheduled):\", avg_wait_scheduled)\n",
        "print(\"Avg wait (walk-in):\", avg_wait_walkin)\n",
        "print(\"Remaining late list:\", len(env.late_list))\n",
        "print(\"Remaining walk-in queue:\", len(env.walkin_queue))\n",
        "\n",
        "# Charts\n",
        "fig1 = px.line(timeline_df, x=\"minute\", y=[\"served\", \"walkin_q\", \"late\"], title=\"Queues and served over time\")\n",
        "fig1.show()\n",
        "\n",
        "if not served_df.empty and served_df[\"wait_minutes\"].notna().any():\n",
        "    fig2 = px.histogram(served_df.dropna(subset=[\"wait_minutes\"]), x=\"wait_minutes\", nbins=30, title=\"Wait time distribution (minutes)\")\n",
        "    fig2.show()\n",
        "else:\n",
        "    print(\"No served patients to plot wait distribution.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1c38302f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d1f42bf"
      },
      "source": [
        "## 7-minute availability booking (user selects an hour)\n",
        "\n",
        "This section adds a simple booking planner so users can pick an hour label based on availability. Each hour has 7-minute sub-slots:\n",
        "- Hours: 08, 09, 10, 11, 13, 14, 15 (12:00–13:00 lunch, closed; 16:00 close)\n",
        "- Capacity rules per hour (7-min consult):\n",
        "  - Normal hours (08, 09, 10, 13, 14): 9 bookings (last may bleed a few minutes into next hour)\n",
        "  - Boundary hours (11 before lunch, 15 before day close): 8 bookings (must finish by 12:00 and 16:00 respectively)\n",
        "- Example behavior: After 9 bookings at 08:00, availability shows 09–12 and 13–16."
      ],
      "id": "1d1f42bf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Env V2: Multi‑provider, variable service times, action masking\n",
        "\n",
        "Enhancements:\n",
        "- Multiple providers can serve patients in parallel (concurrent servers)\n",
        "- Variable service durations (lognormal around 7 minutes, configurable)\n",
        "- Action masking for invalid choices (late recall only if late patient has arrived; etc.)\n",
        "- Optional seeding of scheduled appointments from the 7‑min `BookingPlanner`\n",
        "- Richer logs and provider utilization metrics"
      ],
      "id": "dcbe1045"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from typing import Optional, Dict, Any, List, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "class ClinicSchedulingEnvV2(gym.Env):\n",
        "    \"\"\"Multi-provider scheduling with variable service times and action masking.\"\"\"\n",
        "    metadata = {\"render.modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self,\n",
        "                 slot_minutes: int = 7,\n",
        "                 num_providers: int = 2,\n",
        "                 service_mean_min: float = 7.0,\n",
        "                 service_sigma_min: float = 2.0,\n",
        "                 walkin_rate_per_hour: float = 8.0,\n",
        "                 no_show_prob: float = 0.05,\n",
        "                 late_prob: float = 0.1,\n",
        "                 walkin_cutoff_minute: Optional[int] = None,\n",
        "                 seed: Optional[int] = None,\n",
        "                 seeded_schedule: Optional[List[Tuple[int, int]]] = None  # list of (patient_id, start_minute)\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.slot_minutes = slot_minutes\n",
        "        self.num_providers = num_providers\n",
        "        self.service_mean_min = service_mean_min\n",
        "        self.service_sigma_min = service_sigma_min\n",
        "        self.walkin_rate_per_hour = walkin_rate_per_hour\n",
        "        self.no_show_prob = no_show_prob\n",
        "        self.late_prob = late_prob\n",
        "        self.walkin_cutoff_minute = walkin_cutoff_minute or MINUTES_CLOSE_PM\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        # Action space: 0 scheduled, 1 walk-in, 2 late recall\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        # Observation: [minute_slot, walkin_q, late_len, scheduled_remaining, free_providers, mask0, mask1, mask2]\n",
        "        high = np.array([\n",
        "            WORK_MINUTES // self.slot_minutes,\n",
        "            500,\n",
        "            MAX_SCHEDULED_PER_DAY,\n",
        "            MAX_SCHEDULED_PER_DAY,\n",
        "            self.num_providers,\n",
        "            1, 1, 1\n",
        "        ], dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=0.0, high=high, dtype=np.float32)\n",
        "\n",
        "        # Providers state: remaining service time if busy else 0\n",
        "        self.provider_busy_remaining: List[int] = [0 for _ in range(self.num_providers)]\n",
        "\n",
        "        # Queues\n",
        "        self.walkin_queue: List[Patient] = []\n",
        "        self.late_list: List[Patient] = []\n",
        "        self.scheduled_slots: Dict[int, List[Patient]] = defaultdict(list)  # slot index -> patients\n",
        "        self.served_ids: List[int] = []\n",
        "        self.served_log: List[Dict[str, Any]] = []\n",
        "        self.generated_patients: Dict[int, Patient] = {}\n",
        "        self.minute = MINUTES_OPEN_AM\n",
        "        self.next_walkin_id = 1\n",
        "        self.seeded_schedule = seeded_schedule\n",
        "        self._build_scheduled_from_seed()\n",
        "\n",
        "    def _build_scheduled_from_seed(self):\n",
        "        self.scheduled_slots.clear()\n",
        "        pid = 1\n",
        "        if self.seeded_schedule:\n",
        "            for pid_seed, start_min in self.seeded_schedule:\n",
        "                slot = minute_to_slot(start_min, self.slot_minutes)\n",
        "                arrival = start_min  # assume on-time unless randomized below\n",
        "                if self.rng.random() < self.no_show_prob:\n",
        "                    arrival = None\n",
        "                else:\n",
        "                    # lateness\n",
        "                    if self.rng.random() < self.late_prob:\n",
        "                        arrival = min(start_min + int(self.rng.integers(5, 30)), MINUTES_CLOSE_PM - 1)\n",
        "                p = Patient(id=pid_seed, scheduled_slot=slot, arrival_time_min=arrival)\n",
        "                self.scheduled_slots[slot].append(p)\n",
        "                self.generated_patients[pid_seed] = p\n",
        "                pid = max(pid, pid_seed + 1)\n",
        "        self.next_walkin_id = pid\n",
        "\n",
        "    def _service_duration(self) -> int:\n",
        "        # lognormal-ish clamp around mean\n",
        "        val = max(1.0, self.rng.lognormal(mean=np.log(max(1e-6, self.service_mean_min)), sigma=self.service_sigma_min / max(1.0, self.service_mean_min)))\n",
        "        return int(max(1, round(val)))\n",
        "\n",
        "    def _maybe_generate_walkins(self):\n",
        "        if not is_open_minute(self.minute) or self.minute >= self.walkin_cutoff_minute:\n",
        "            return\n",
        "        lam = self.walkin_rate_per_hour / 60.0\n",
        "        arrivals = self.rng.poisson(lam)\n",
        "        for _ in range(arrivals):\n",
        "            p = Patient(id=self.next_walkin_id, scheduled_slot=None, arrival_time_min=self.minute)\n",
        "            self.walkin_queue.append(p)\n",
        "            self.generated_patients[p.id] = p\n",
        "            self.next_walkin_id += 1\n",
        "\n",
        "    def _update_late_status(self):\n",
        "        for slot, patients in list(self.scheduled_slots.items()):\n",
        "            slot_minute = self._slot_to_minute(slot)\n",
        "            for p in patients:\n",
        "                if p.arrival_time_min is None:\n",
        "                    continue\n",
        "                if p.arrival_time_min > self.minute and self.minute >= slot_minute:\n",
        "                    p.is_late = True\n",
        "                    if p not in self.late_list and p.id not in self.served_ids and self.minute >= slot_minute:\n",
        "                        self.late_list.append(p)\n",
        "\n",
        "    def _slot_to_minute(self, slot_index: int) -> int:\n",
        "        am_slots = (MINUTES_LUNCH_START - MINUTES_OPEN_AM) // self.slot_minutes\n",
        "        if slot_index < am_slots:\n",
        "            return MINUTES_OPEN_AM + slot_index * self.slot_minutes\n",
        "        else:\n",
        "            return MINUTES_LUNCH_END + (slot_index - am_slots) * self.slot_minutes\n",
        "\n",
        "    def _mask(self) -> np.ndarray:\n",
        "        mask = np.array([0, 0, 0], dtype=np.int8)\n",
        "        # scheduled on-site available?\n",
        "        slot, patient = self._next_on_site_patient()\n",
        "        if patient is not None:\n",
        "            mask[0] = 1\n",
        "        # walk-in available?\n",
        "        if len(self.walkin_queue) > 0:\n",
        "            mask[1] = 1\n",
        "        # late recall available (arrived late and in list)?\n",
        "        if any(p.arrival_time_min is not None and p.arrival_time_min <= self.minute for p in self.late_list):\n",
        "            mask[2] = 1\n",
        "        # if no providers free, no actions available\n",
        "        if self.free_providers() == 0:\n",
        "            mask[:] = 0\n",
        "        return mask\n",
        "\n",
        "    def _remaining_scheduled(self):\n",
        "        for slot in sorted(self.scheduled_slots.keys()):\n",
        "            for patient in self.scheduled_slots[slot]:\n",
        "                if patient.id not in self.served_ids:\n",
        "                    yield slot, patient\n",
        "\n",
        "    def _remaining_scheduled_count(self) -> int:\n",
        "        return sum(1 for _ in self._remaining_scheduled())\n",
        "\n",
        "    def _next_on_site_patient(self):\n",
        "        for slot, patient in self._remaining_scheduled():\n",
        "            if patient.arrival_time_min is not None and patient.arrival_time_min <= self.minute:\n",
        "                return slot, patient\n",
        "        return None, None\n",
        "\n",
        "    def free_providers(self) -> int:\n",
        "        return sum(1 for t in self.provider_busy_remaining if t <= 0)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = np.array([\n",
        "            minute_to_slot(self.minute, self.slot_minutes),\n",
        "            len(self.walkin_queue),\n",
        "            len(self.late_list),\n",
        "            self._remaining_scheduled_count(),\n",
        "            self.free_providers(),\n",
        "            *self._mask().tolist()\n",
        "        ], dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "    def step(self, action: int):\n",
        "        reward = 0.0\n",
        "        info: Dict[str, Any] = {}\n",
        "\n",
        "        # process arrivals and late updates\n",
        "        self._maybe_generate_walkins()\n",
        "        self._update_late_status()\n",
        "\n",
        "        served_patients: List[Patient] = []\n",
        "        mask = self._mask()\n",
        "        # serve up to number of free providers, consistent with intended action preference\n",
        "        to_serve = self.free_providers()\n",
        "        for _ in range(to_serve):\n",
        "            candidate: Optional[Patient] = None\n",
        "            if action == 2 and mask[2]:\n",
        "                # serve arrived late first\n",
        "                for i, p in enumerate(self.late_list):\n",
        "                    if p.arrival_time_min is not None and p.arrival_time_min <= self.minute:\n",
        "                        candidate = self.late_list.pop(i)\n",
        "                        break\n",
        "            if candidate is None and action == 0 and mask[0]:\n",
        "                _, on_site = self._next_on_site_patient()\n",
        "                if on_site is not None:\n",
        "                    candidate = on_site\n",
        "            if candidate is None and action in (1, 0, 2) and mask[1] and self.walkin_queue:\n",
        "                candidate = self.walkin_queue.pop(0)\n",
        "            if candidate is not None:\n",
        "                self.served_ids.append(candidate.id)\n",
        "                served_patients.append(candidate)\n",
        "                # assign to a provider\n",
        "                for i in range(self.num_providers):\n",
        "                    if self.provider_busy_remaining[i] <= 0:\n",
        "                        self.provider_busy_remaining[i] = self._service_duration()\n",
        "                        break\n",
        "\n",
        "        # reward and logs\n",
        "        reward += 1.0 * len(served_patients)\n",
        "        for p in served_patients:\n",
        "            wait = None\n",
        "            if p.arrival_time_min is not None:\n",
        "                wait = max(0, self.minute - p.arrival_time_min)\n",
        "            self.served_log.append({\n",
        "                \"id\": p.id,\n",
        "                \"served_minute\": self.minute,\n",
        "                \"arrival_time\": p.arrival_time_min,\n",
        "                \"is_walkin\": p.scheduled_slot is None,\n",
        "                \"is_late\": bool(p.is_late),\n",
        "                \"wait_minutes\": wait,\n",
        "            })\n",
        "            if p.scheduled_slot is not None:\n",
        "                reward += 0.05\n",
        "\n",
        "        # idle penalty if queues but no serve due to mask or no providers free\n",
        "        if len(served_patients) == 0 and (self.walkin_queue or self.late_list or self._remaining_scheduled_count() > 0):\n",
        "            reward -= 0.01\n",
        "\n",
        "        # advance time by 1 minute; decrement providers\n",
        "        self.minute += 1\n",
        "        for i in range(self.num_providers):\n",
        "            if self.provider_busy_remaining[i] > 0:\n",
        "                self.provider_busy_remaining[i] -= 1\n",
        "        if MINUTES_LUNCH_START <= self.minute < MINUTES_LUNCH_END:\n",
        "            self.minute = MINUTES_LUNCH_END\n",
        "        done = self.minute >= MINUTES_CLOSE_PM\n",
        "\n",
        "        # terminal penalty for unserved scheduled\n",
        "        if done:\n",
        "            remaining_sched = self._remaining_scheduled_count()\n",
        "            reward -= 0.1 * remaining_sched\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        return obs, reward, done, False, info\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "        self.minute = MINUTES_OPEN_AM\n",
        "        self.provider_busy_remaining = [0 for _ in range(self.num_providers)]\n",
        "        self.walkin_queue = []\n",
        "        self.late_list = []\n",
        "        self.served_ids = []\n",
        "        self.served_log = []\n",
        "        self.generated_patients = {}\n",
        "        self._build_scheduled_from_seed()\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def render(self):\n",
        "        print({\n",
        "            \"minute\": self.minute,\n",
        "            \"free_providers\": self.free_providers(),\n",
        "            \"walkin_q\": len(self.walkin_queue),\n",
        "            \"late\": len(self.late_list)\n",
        "        })"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7cd71d6a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calibration/config cell for Env V2\n",
        "CALIB = {\n",
        "    \"slot_minutes\": 7,\n",
        "    \"num_providers\": 2,\n",
        "    \"service_mean_min\": 7.0,\n",
        "    \"service_sigma_min\": 2.0,\n",
        "    \"walkin_rate_per_hour\": 10.0,\n",
        "    \"no_show_prob\": 0.07,\n",
        "    \"late_prob\": 0.12,\n",
        "    \"walkin_cutoff_minute\": MINUTES_CLOSE_PM,\n",
        "}\n",
        "\n",
        "# Example: seed scheduled patients from BookingPlanner (first N bookings at 8:00 then 9:00)\n",
        "seed_schedule = []  # list of (patient_id, start_minute)\n",
        "if \"BookingPlanner\" in globals():\n",
        "    planner = BookingPlanner()\n",
        "    patient_id = 1\n",
        "    for hour in [8, 9]:\n",
        "        for _ in range(5):  # first 5 bookings per hour for demo\n",
        "            booked_label = planner.book(hour)\n",
        "            if booked_label is None:\n",
        "                break\n",
        "            slot_index = planner.booked_count_by_hour[hour] - 1\n",
        "            minute = hour * 60 + slot_index * planner.slot_minutes\n",
        "            seed_schedule.append((patient_id, minute))\n",
        "            patient_id += 1\n",
        "else:\n",
        "    print(\"BookingPlanner not yet defined; using deterministic slot seeding below.\")\n",
        "\n",
        "# Fallback: deterministic seed schedule at minute marks if planner wasn't available or yielded no seeds\n",
        "if not seed_schedule:\n",
        "    seed_schedule = [(i + 1, 8 * 60 + i * CALIB[\"slot_minutes\"]) for i in range(9)]  # 8:00 to ~8:56\n",
        "    seed_schedule += [(10 + i, 9 * 60 + i * CALIB[\"slot_minutes\"]) for i in range(5)]\n",
        "\n",
        "print(\"Seeds (first 3):\", seed_schedule[:3])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2cc38932"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training with MaskablePPO if available, else PPO\n",
        "from stable_baselines3 import PPO\n",
        "try:\n",
        "    from sb3_contrib import MaskablePPO\n",
        "    from sb3_contrib.common.wrappers import ActionMasker\n",
        "    MASKABLE = True\n",
        "except Exception:\n",
        "    MASKABLE = False\n",
        "    ActionMasker = None\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "import os\n",
        "\n",
        "log_dir_v2 = \"/content/logs_v2\" if os.path.exists(\"/content\") else \"/workspace/logs_v2\"\n",
        "os.makedirs(log_dir_v2, exist_ok=True)\n",
        "\n",
        "# build env factory with seeded schedule\n",
        "\n",
        "def make_env_v2():\n",
        "    return ClinicSchedulingEnvV2(\n",
        "        slot_minutes=CALIB[\"slot_minutes\"],\n",
        "        num_providers=CALIB[\"num_providers\"],\n",
        "        service_mean_min=CALIB[\"service_mean_min\"],\n",
        "        service_sigma_min=CALIB[\"service_sigma_min\"],\n",
        "        walkin_rate_per_hour=CALIB[\"walkin_rate_per_hour\"],\n",
        "        no_show_prob=CALIB[\"no_show_prob\"],\n",
        "        late_prob=CALIB[\"late_prob\"],\n",
        "        walkin_cutoff_minute=CALIB[\"walkin_cutoff_minute\"],\n",
        "        seeded_schedule=seed_schedule,\n",
        "    )\n",
        "\n",
        "\n",
        "def _mask_fn(env: ClinicSchedulingEnvV2):\n",
        "    return env._mask().astype(bool)\n",
        "\n",
        "\n",
        "def make_env_v2_masked():\n",
        "    base = make_env_v2()\n",
        "    return ActionMasker(base, _mask_fn)\n",
        "\n",
        "env_factory = make_env_v2_masked if MASKABLE and ActionMasker is not None else make_env_v2\n",
        "\n",
        "vec_env_v2 = make_vec_env(env_factory, n_envs=1, monitor_dir=log_dir_v2)\n",
        "\n",
        "if MASKABLE:\n",
        "    model_v2 = MaskablePPO(\n",
        "        \"MlpPolicy\",\n",
        "        vec_env_v2,\n",
        "        verbose=1,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=256,\n",
        "        n_epochs=10,\n",
        "        gamma=0.995,\n",
        "        gae_lambda=0.95,\n",
        "    )\n",
        "else:\n",
        "    model_v2 = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        vec_env_v2,\n",
        "        verbose=1,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=256,\n",
        "        n_epochs=10,\n",
        "        gamma=0.995,\n",
        "        gae_lambda=0.95,\n",
        "    )\n",
        "\n",
        "# Optional evaluation callback\n",
        "# eval_env_v2 = make_vec_env(make_env_v2, n_envs=1)\n",
        "# eval_cb = EvalCallback(eval_env_v2, best_model_save_path=log_dir_v2, log_path=log_dir_v2, eval_freq=10_000)\n",
        "\n",
        "timesteps_v2 = 200_000\n",
        "model_v2.learn(total_timesteps=timesteps_v2)  # , callback=eval_cb)\n",
        "\n",
        "model_v2_path = os.path.join(log_dir_v2, \"model_v2\")\n",
        "model_v2.save(model_v2_path)\n",
        "print(\"Saved V2 model to\", model_v2_path)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "711a1fb1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluation for Env V2\n",
        "import pandas as pd\n",
        "from statistics import mean\n",
        "\n",
        "loaded_v2 = None\n",
        "try:\n",
        "    from sb3_contrib import MaskablePPO\n",
        "    loaded_v2 = MaskablePPO.load(model_v2_path)\n",
        "except Exception:\n",
        "    from stable_baselines3 import PPO\n",
        "    loaded_v2 = PPO.load(model_v2_path)\n",
        "\n",
        "\n",
        "def evaluate_v2(episodes=3):\n",
        "    metrics = []\n",
        "    for ep in range(episodes):\n",
        "        env = make_env_v2()\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, _ = loaded_v2.predict(obs, deterministic=True)\n",
        "            obs, reward, terminated, truncated, _ = env.step(int(action))\n",
        "            done = bool(terminated or truncated)\n",
        "        # compute metrics\n",
        "        served_df = pd.DataFrame(env.served_log)\n",
        "        scheduled_served = int((served_df[\"is_walkin\"] == False).sum()) if not served_df.empty else 0\n",
        "        walkin_served = int((served_df[\"is_walkin\"] == True).sum()) if not served_df.empty else 0\n",
        "        avg_wait = float(served_df[\"wait_minutes\"].dropna().mean()) if not served_df.empty else float(\"nan\")\n",
        "        util = 1.0 - (sum(1 for t in env.provider_busy_remaining if t <= 0) / env.num_providers)\n",
        "        metrics.append({\n",
        "            \"scheduled_served\": scheduled_served,\n",
        "            \"walkin_served\": walkin_served,\n",
        "            \"avg_wait\": avg_wait,\n",
        "            \"late_remaining\": len(env.late_list),\n",
        "            \"walkins_remaining\": len(env.walkin_queue),\n",
        "            \"providers\": env.num_providers,\n",
        "        })\n",
        "    return pd.DataFrame(metrics)\n",
        "\n",
        "m = evaluate_v2(episodes=3)\n",
        "print(m.describe(include=\"all\"))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f9adcbb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data-driven calibration from CSV logs\n",
        "Provide CSVs with historical data to fit key parameters:\n",
        "- arrivals.csv: columns [timestamp, type(scheduled|walkin), booked_minute, arrival_minute, no_show(0/1), late(0/1)]\n",
        "- service.csv: columns [timestamp, provider_id, service_minutes]\n",
        "\n",
        "This cell loads CSVs (if provided) from Colab/Drive or local path and estimates walk-in hourly rates by time-of-day, no-show and late probabilities, and service-time distribution parameters. Fallbacks are used if files are absent."
      ],
      "id": "18e73caf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calibration from CSV logs (optional)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import log\n",
        "\n",
        "# Set these paths in Colab or mount Drive\n",
        "ARRIVALS_CSV = None  # e.g., '/content/arrivals.csv'\n",
        "SERVICE_CSV = None   # e.g., '/content/service.csv'\n",
        "\n",
        "calib = CALIB.copy()\n",
        "try:\n",
        "    if ARRIVALS_CSV:\n",
        "        arr = pd.read_csv(ARRIVALS_CSV)\n",
        "        # Time-of-day walk-in rate estimation\n",
        "        arr_walk = arr[arr['type'].str.lower() == 'walkin'].copy()\n",
        "        arr_walk['hour'] = (arr_walk['arrival_minute'] // 60).astype(int)\n",
        "        hourly_counts = arr_walk.groupby('hour').size()\n",
        "        # default to mean if missing hours\n",
        "        default_rate = hourly_counts.mean() if len(hourly_counts) else calib['walkin_rate_per_hour']\n",
        "        walkin_rate_by_hour = {h: hourly_counts.get(h, default_rate) for h in [8,9,10,11,13,14,15]}\n",
        "        # normalize to per-hour arrival rate\n",
        "        calib['walkin_rate_per_hour'] = float(np.mean(list(walkin_rate_by_hour.values())))\n",
        "        # No-show and late\n",
        "        sched = arr[arr['type'].str.lower() == 'scheduled']\n",
        "        if len(sched) > 0:\n",
        "            calib['no_show_prob'] = float((sched['no_show'] == 1).mean())\n",
        "            calib['late_prob'] = float((sched['late'] == 1).mean())\n",
        "    if SERVICE_CSV:\n",
        "        svc = pd.read_csv(SERVICE_CSV)\n",
        "        durations = svc['service_minutes'].dropna().values\n",
        "        if len(durations) > 5:\n",
        "            calib['service_mean_min'] = float(np.mean(durations))\n",
        "            # set sigma as std/mean capped\n",
        "            std = float(np.std(durations))\n",
        "            calib['service_sigma_min'] = float(min(std, calib['service_mean_min']))\n",
        "    print('Calibrated parameters:', calib)\n",
        "except Exception as e:\n",
        "    print('Calibration failed, using defaults. Reason:', e)\n",
        "    calib = CALIB.copy()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2d5c8b9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rule baseline and safety wrapper\n",
        "- Rule baseline: a straightforward heuristic policy (serve priority order: late recall if arrived > scheduled on-site > walk-in). Supports multi-provider.\n",
        "- OR-Tools (optional): If installed, can compute a simple re-optimization for remaining scheduled patients under time windows (demonstration-level).\n",
        "- Safety wrapper: masks invalid actions, enforces hard constraints (no service during lunch/after close), and provides a deterministic fallback if the RL suggests an invalid action."
      ],
      "id": "182ff9ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Heuristic baseline and optional OR-Tools demo\n",
        "from typing import Optional\n",
        "\n",
        "class HeuristicPolicy:\n",
        "    def select_action(self, env: ClinicSchedulingEnvV2) -> int:\n",
        "        base_env = env.unwrapped if hasattr(env, \"unwrapped\") else env\n",
        "        if not hasattr(base_env, \"_mask\"):\n",
        "            raise AttributeError(\"Underlying environment must expose _mask()\")\n",
        "        mask = base_env._mask()\n",
        "        if mask[2]:\n",
        "            return 2  # recall late\n",
        "        if mask[0]:\n",
        "            return 0  # serve scheduled on-site\n",
        "        if mask[1]:\n",
        "            return 1  # serve walk-in\n",
        "        return 1  # default (no-ops will idle)\n",
        "\n",
        "# Safety wrapper for RL decisions\n",
        "class SafePolicy:\n",
        "    def __init__(self, env: ClinicSchedulingEnvV2, rl_model):\n",
        "        self.env = env\n",
        "        self.rl_model = rl_model\n",
        "        self.fallback = HeuristicPolicy()\n",
        "\n",
        "    def predict(self, obs, deterministic: bool = True) -> tuple[int, None]:\n",
        "        # Allow same signature as SB3 models and return (action, None)\n",
        "        try:\n",
        "            action, _ = self.rl_model.predict(obs, deterministic=deterministic)\n",
        "            action = int(action)\n",
        "        except Exception:\n",
        "            action = self.fallback.select_action(self.env)\n",
        "        # mask invalid\n",
        "        mask = self.env._mask()\n",
        "        if action < 0 or action > 2 or mask[action] == 0:\n",
        "            action = self.fallback.select_action(self.env)\n",
        "        return action, None\n",
        "\n",
        "# OR-Tools mini-demo (optional)\n",
        "try:\n",
        "    from ortools.sat.python import cp_model\n",
        "    HAS_OR_TOOLS = True\n",
        "except Exception:\n",
        "    HAS_OR_TOOLS = False\n",
        "\n",
        "\n",
        "def reoptimize_schedule_demo(env: ClinicSchedulingEnvV2):\n",
        "    if not HAS_OR_TOOLS:\n",
        "        print(\"OR-Tools not installed; skipping demo.\")\n",
        "        return None\n",
        "    # Build a minimal CP-SAT to assign remaining scheduled patients to time slots after now\n",
        "    model = cp_model.CpModel()\n",
        "    remaining = [\n",
        "        (slot, patient)\n",
        "        for slot, patients in env.scheduled_slots.items()\n",
        "        for patient in patients\n",
        "        if patient.id not in env.served_ids\n",
        "    ]\n",
        "    if not remaining:\n",
        "        return None\n",
        "    slots = [s for s, _ in remaining]\n",
        "    # binary vars: assign each to current slot or next few slots\n",
        "    vars = {}\n",
        "    for idx, (s, p) in enumerate(remaining):\n",
        "        for delta in range(0, 5):\n",
        "            ss = s + delta\n",
        "            vars[(idx, ss)] = model.NewBoolVar(f\"x_{idx}_{ss}\")\n",
        "        # each patient assigned once\n",
        "        model.Add(sum(vars[(idx, s + d)] for d in range(0, 5)) == 1)\n",
        "    # capacity: per slot up to number of providers\n",
        "    for ss in range(min(slots), max(slots) + 5):\n",
        "        model.Add(sum(vars[(idx, ss)] for idx, _ in enumerate(remaining) if (idx, ss) in vars) <= env.num_providers)\n",
        "    model.Minimize(0)\n",
        "    solver = cp_model.CpSolver()\n",
        "    solver.parameters.max_time_in_seconds = 2.0\n",
        "    solver.Solve(model)\n",
        "    assignment = {idx: None for idx in range(len(remaining))}\n",
        "    for key, var in vars.items():\n",
        "        if solver.Value(var) == 1:\n",
        "            idx, ss = key\n",
        "            assignment[idx] = ss\n",
        "    return assignment"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "407a2c9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Off-policy evaluation (OPE) and backtesting\n",
        "Given historical logs with state-action pairs and outcomes, estimate policy performance without deploying it:\n",
        "- Replay logged trajectories to compute value under baseline policy.\n",
        "- Importance sampling/weighted importance sampling (WIS) using a logged behavior policy if known.\n",
        "- For simplicity here, we backtest the heuristic and RL policies on multiple simulated days and compare metrics to logs when available."
      ],
      "id": "768afd4d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple backtest runner comparing Heuristic vs RL (simulated OPE)\n",
        "import pandas as pd\n",
        "\n",
        "def run_policy(env_fn, policy, episodes=10):\n",
        "    rows = []\n",
        "    for ep in range(episodes):\n",
        "        env = env_fn()\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        served = 0\n",
        "        while not done:\n",
        "            if isinstance(policy, HeuristicPolicy):\n",
        "                action = policy.select_action(env)\n",
        "            else:\n",
        "                action, _ = policy.predict(obs, deterministic=True)\n",
        "                action = int(action)\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = bool(terminated or truncated)\n",
        "            steps += 1\n",
        "        served_df = pd.DataFrame(env.served_log)\n",
        "        served_total = len(served_df)\n",
        "        avg_wait = float(served_df['wait_minutes'].dropna().mean()) if not served_df.empty else float('nan')\n",
        "        rows.append({\n",
        "            'served_total': served_total,\n",
        "            'avg_wait': avg_wait,\n",
        "            'late_remaining': len(env.late_list),\n",
        "            'walkins_remaining': len(env.walkin_queue),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "heur = HeuristicPolicy()\n",
        "heur_df = run_policy(make_env_v2, heur, episodes=5)\n",
        "print('Heuristic summary:')\n",
        "print(heur_df.describe())\n",
        "\n",
        "safe_rl = SafePolicy(make_env_v2(), loaded_v2)\n",
        "rl_df = run_policy(make_env_v2, safe_rl, episodes=5)\n",
        "print('RL summary:')\n",
        "print(rl_df.describe())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "aecfac9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Domain randomization\n",
        "During training, randomize environment parameters each episode to improve robustness (sim-to-real). This wrapper perturbs arrival rates, no-show/late probabilities, and service-time parameters within bounds."
      ],
      "id": "b237427a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Domain randomization wrapper\n",
        "class DomainRandomizedFactory:\n",
        "    def __init__(self, base_calib: dict, ranges: dict):\n",
        "        self.base = base_calib\n",
        "        self.ranges = ranges\n",
        "\n",
        "    def sample(self) -> dict:\n",
        "        cfg = self.base.copy()\n",
        "        rng = np.random.default_rng()\n",
        "        for k, (low, high) in self.ranges.items():\n",
        "            if isinstance(low, float) or isinstance(high, float):\n",
        "                cfg[k] = float(rng.uniform(low, high))\n",
        "            else:\n",
        "                cfg[k] = int(rng.integers(low, high + 1))\n",
        "        return cfg\n",
        "\n",
        "    def make_env(self):\n",
        "        cfg = self.sample()\n",
        "        return ClinicSchedulingEnvV2(\n",
        "            slot_minutes=cfg.get('slot_minutes', CALIB['slot_minutes']),\n",
        "            num_providers=cfg.get('num_providers', CALIB['num_providers']),\n",
        "            service_mean_min=cfg.get('service_mean_min', CALIB['service_mean_min']),\n",
        "            service_sigma_min=cfg.get('service_sigma_min', CALIB['service_sigma_min']),\n",
        "            walkin_rate_per_hour=cfg.get('walkin_rate_per_hour', CALIB['walkin_rate_per_hour']),\n",
        "            no_show_prob=cfg.get('no_show_prob', CALIB['no_show_prob']),\n",
        "            late_prob=cfg.get('late_prob', CALIB['late_prob']),\n",
        "            walkin_cutoff_minute=cfg.get('walkin_cutoff_minute', CALIB['walkin_cutoff_minute']),\n",
        "            seeded_schedule=seed_schedule,\n",
        "        )\n",
        "\n",
        "# Example ranges for randomization\n",
        "RAND_RANGES = {\n",
        "    'walkin_rate_per_hour': (6.0, 14.0),\n",
        "    'no_show_prob': (0.03, 0.12),\n",
        "    'late_prob': (0.05, 0.2),\n",
        "    'service_mean_min': (6.0, 10.0),\n",
        "    'service_sigma_min': (1.5, 3.0),\n",
        "}\n",
        "\n",
        "rand_factory = DomainRandomizedFactory(calib, RAND_RANGES)\n",
        "\n",
        "# Train with randomization (short run)\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "vec_env_rand = make_vec_env(rand_factory.make_env, n_envs=1)\n",
        "model_rand = PPO('MlpPolicy', vec_env_rand, verbose=1)\n",
        "model_rand.learn(total_timesteps=100_000)\n",
        "print('Domain-randomized model trained.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b1ed8faa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shadow-mode integration scaffold (FastAPI)\n",
        "This example shows how to serve availability and decisions in shadow mode: the system returns rule-based decisions to operators while logging RL suggestions for comparison. In Colab, this is illustrative; deploy as a microservice in production."
      ],
      "id": "8b65b572"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# FastAPI shadow-mode scaffold (illustrative)\n",
        "try:\n",
        "    from fastapi import FastAPI\n",
        "    from pydantic import BaseModel\n",
        "    import uvicorn\n",
        "\n",
        "    app = FastAPI()\n",
        "\n",
        "    class DecisionRequest(BaseModel):\n",
        "        obs: list\n",
        "\n",
        "    class DecisionResponse(BaseModel):\n",
        "        action_rule: int\n",
        "        action_rl: int\n",
        "        mask: list\n",
        "\n",
        "    # Build single env for masking and a safe policy wrapper\n",
        "    env_for_api = make_env_v2()\n",
        "    safe_rl_policy = SafePolicy(env_for_api, loaded_v2)\n",
        "    heuristic_policy = HeuristicPolicy()\n",
        "\n",
        "    @app.get(\"/availability\")\n",
        "    def availability():\n",
        "        # Return hour labels from the planner demo\n",
        "        planner = BookingPlanner()\n",
        "        return {\"availability\": planner.availability_label()}\n",
        "\n",
        "    @app.post(\"/decide\", response_model=DecisionResponse)\n",
        "    def decide(req: DecisionRequest):\n",
        "        obs = np.array(req.obs, dtype=np.float32)\n",
        "        # Rule action\n",
        "        action_rule = heuristic_policy.select_action(env_for_api)\n",
        "        # RL suggestion (masked)\n",
        "        action_rl = safe_rl_policy.predict(obs)\n",
        "        return DecisionResponse(action_rule=action_rule, action_rl=action_rl, mask=env_for_api._mask().tolist())\n",
        "\n",
        "    print(\"To run locally (not in Colab runtime):\\nuvicorn main:app --reload\")\n",
        "except Exception as e:\n",
        "    print(\"FastAPI not available or running in limited environment:\", e)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "60f2bc2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MaskablePPO with Action Masking + Reward Shaping\n",
        "Train with action masking and a shaping wrapper to improve learning signal and safety. Requires `sb3-contrib` for MaskablePPO and ActionMasker."
      ],
      "id": "97749858"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ActionMasker + Reward shaping wrappers\n",
        "try:\n",
        "    from sb3_contrib.ppo_mask import MaskablePPO\n",
        "    from sb3_contrib.common.wrappers import ActionMasker\n",
        "    HAS_MASKABLE = True\n",
        "except Exception:\n",
        "    HAS_MASKABLE = False\n",
        "\n",
        "class RewardShapingWrapper(gym.Wrapper):\n",
        "    def __init__(self, env: ClinicSchedulingEnvV2):\n",
        "        super().__init__(env)\n",
        "        self.env: ClinicSchedulingEnvV2\n",
        "    def step(self, action):\n",
        "        obs, reward, done, trunc, info = self.env.step(action)\n",
        "        # Encourage serving scheduled first when on-site\n",
        "        mask = self.env._mask()\n",
        "        if mask[0] and action == 0:\n",
        "            reward += 0.02\n",
        "        # Penalize long waits more strongly\n",
        "        if self.env.served_log:\n",
        "            w = self.env.served_log[-1].get('wait_minutes')\n",
        "            if w is not None and w > 30:\n",
        "                reward -= 0.02\n",
        "        return obs, reward, done, trunc, info\n",
        "\n",
        "\n",
        "def mask_fn(env: ClinicSchedulingEnvV2):\n",
        "    return env._mask().astype(bool)\n",
        "\n",
        "\n",
        "def make_masked_env():\n",
        "    base = make_env_v2()\n",
        "    shaped = RewardShapingWrapper(base)\n",
        "    if HAS_MASKABLE:\n",
        "        return ActionMasker(shaped, mask_fn)\n",
        "    return shaped\n",
        "\n",
        "if HAS_MASKABLE:\n",
        "    vec_masked = make_vec_env(make_masked_env, n_envs=1)\n",
        "    model_masked = MaskablePPO('MlpPolicy', vec_masked, verbose=1)\n",
        "    model_masked.learn(total_timesteps=200_000)\n",
        "    print('MaskablePPO training complete.')\n",
        "else:\n",
        "    print('sb3-contrib not available; skipping MaskablePPO training.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f4e902ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optuna hyperparameter tuning (optional)\n",
        "Run a short study to search PPO/MaskablePPO hyperparameters. May be compute-intensive in Colab."
      ],
      "id": "f426ed99"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optuna tuning demo\n",
        "try:\n",
        "    import optuna\n",
        "    HAS_OPTUNA = True\n",
        "except Exception:\n",
        "    HAS_OPTUNA = False\n",
        "\n",
        "if HAS_OPTUNA:\n",
        "    def objective(trial):\n",
        "        lr = trial.suggest_float('lr', 1e-5, 5e-4, log=True)\n",
        "        n_steps = trial.suggest_int('n_steps', 512, 4096, log=True)\n",
        "        batch_size = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
        "        gamma = trial.suggest_float('gamma', 0.98, 0.999)\n",
        "\n",
        "        env = make_masked_env() if HAS_MASKABLE else make_env_v2()\n",
        "        from stable_baselines3 import PPO\n",
        "        model = (MaskablePPO('MlpPolicy', env, verbose=0, learning_rate=lr, n_steps=n_steps, batch_size=batch_size, gamma=gamma)\n",
        "                 if HAS_MASKABLE else\n",
        "                 PPO('MlpPolicy', env, verbose=0, learning_rate=lr, n_steps=n_steps, batch_size=batch_size, gamma=gamma))\n",
        "        model.learn(total_timesteps=30_000)\n",
        "\n",
        "        # Evaluate quickly\n",
        "        def eval_once():\n",
        "            e = make_env_v2()\n",
        "            obs, _ = e.reset()\n",
        "            done = False\n",
        "            served = 0\n",
        "            while not done:\n",
        "                a, _ = model.predict(obs, deterministic=True)\n",
        "                obs, r, term, trunc, _ = e.step(int(a))\n",
        "                done = bool(term or trunc)\n",
        "            return len(e.served_log)\n",
        "        return eval_once()\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=10)\n",
        "    print('Best params:', study.best_params)\n",
        "else:\n",
        "    print('Optuna not installed; skipping tuning.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "687c7ac5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical evaluation with confidence intervals\n",
        "Compute means and 95% CIs for key metrics (served_total, avg_wait) across multiple episodes to quantify reliability."
      ],
      "id": "b5eef919"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Statistical evaluation (95% CI)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "def ci95(series: pd.Series):\n",
        "    x = series.dropna().values\n",
        "    if len(x) < 2:\n",
        "        return (float('nan'), float('nan'))\n",
        "    mean = float(np.mean(x))\n",
        "    se = float(np.std(x, ddof=1) / sqrt(len(x)))\n",
        "    return (mean - 1.96 * se, mean + 1.96 * se)\n",
        "\n",
        "# Example: compare heuristic vs RL with CIs\n",
        "heur_df = run_policy(make_env_v2, HeuristicPolicy(), episodes=20)\n",
        "print('Heuristic served_total mean, 95% CI:', np.mean(heur_df['served_total']), ci95(heur_df['served_total']))\n",
        "print('Heuristic avg_wait mean, 95% CI:', np.mean(heur_df['avg_wait']), ci95(heur_df['avg_wait']))\n",
        "\n",
        "safe_rl = SafePolicy(make_env_v2(), loaded_v2)\n",
        "rl_df = run_policy(make_env_v2, safe_rl, episodes=20)\n",
        "print('RL served_total mean, 95% CI:', np.mean(rl_df['served_total']), ci95(rl_df['served_total']))\n",
        "print('RL avg_wait mean, 95% CI:', np.mean(rl_df['avg_wait']), ci95(rl_df['avg_wait']))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "26d8e87d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Clinic Logs\n",
        "Use this section to connect Google Drive (in Colab) or point to locally available CSV files that contain historical scheduling data. The calibration cells below expect arrival/service files in the formats described here."
      ],
      "id": "c1017814"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configure data sources for calibration\n",
        "import pathlib\n",
        "\n",
        "# --- Step 1: (Optional) Mount Google Drive when running in Colab ---\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    DRIVE_MOUNT = pathlib.Path('/content/drive')\n",
        "    if not DRIVE_MOUNT.exists():\n",
        "        drive.mount('/content/drive')\n",
        "    print('Drive mounted at /content/drive')\n",
        "except Exception:\n",
        "    print('Google Drive not detected; skipping mount. Set paths manually below.')\n",
        "\n",
        "# --- Step 2: Set file paths for historical data ---\n",
        "# Replace these with the location of your exported logs.\n",
        "# Expectation:\n",
        "#   ARRIVALS_CSV: each row has booked_minute (int), timestamp (datetime), arrival_minute,\n",
        "#                 no_show (0/1), late (0/1), and any categorical features.\n",
        "#   SERVICE_CSV: each row has service_start_minute, service_duration_min, provider_id, etc.\n",
        "ARRIVALS_CSV = ARRIVALS_CSV if 'ARRIVALS_CSV' in globals() else ''\n",
        "SERVICE_CSV = SERVICE_CSV if 'SERVICE_CSV' in globals() else ''\n",
        "\n",
        "# Example (uncomment and edit):\n",
        "# ARRIVALS_CSV = '/content/drive/MyDrive/clinic_logs/arrivals.csv'\n",
        "# SERVICE_CSV = '/content/drive/MyDrive/clinic_logs/service_times.csv'\n",
        "\n",
        "print('ARRIVALS_CSV =', ARRIVALS_CSV or '(not set)')\n",
        "print('SERVICE_CSV  =', SERVICE_CSV or '(not set)')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "35cd1158"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick preview of data (optional)\n",
        "import pandas as pd\n",
        "\n",
        "def preview_csv(path, n=5):\n",
        "    if not path:\n",
        "        print('Path is empty; skip preview.')\n",
        "        return\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        print(f'{path} -> {len(df)} rows')\n",
        "        display(df.head(n))\n",
        "    except FileNotFoundError:\n",
        "        print(f'File not found: {path}')\n",
        "    except Exception as exc:\n",
        "        print(f'Could not read {path}: {exc}')\n",
        "\n",
        "preview_csv(ARRIVALS_CSV)\n",
        "preview_csv(SERVICE_CSV)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ba4c08e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full calibration pipeline and tuned hyperparameters\n",
        "This section fits no-show and late-arrival models from CSV logs, calibrates environment parameters and domain-randomization ranges, and runs a short Optuna study to output tuned hyperparameters. Provide paths in `ARRIVALS_CSV` and `SERVICE_CSV`, then run the cells."
      ],
      "id": "907fad1e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit no-show and late models (logistic) + set calibrated params\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "ARRIVALS_CSV = ARRIVALS_CSV or None\n",
        "SERVICE_CSV = SERVICE_CSV or None\n",
        "\n",
        "noshow_model = None\n",
        "late_model = None\n",
        "\n",
        "try:\n",
        "    if ARRIVALS_CSV:\n",
        "        df = pd.read_csv(ARRIVALS_CSV)\n",
        "        # Basic feature set: hour, dow, visit_type (if present)\n",
        "        df['hour'] = (df['booked_minute'] // 60).astype(int)\n",
        "        df['dow'] = pd.to_datetime(df['timestamp']).dt.dayofweek\n",
        "        feature_cols = ['hour', 'dow'] + ([ 'visit_type' ] if 'visit_type' in df.columns else [])\n",
        "        X = df[feature_cols]\n",
        "        # No-show model (scheduled only)\n",
        "        sched = df[df['type'].str.lower() == 'scheduled'].copy()\n",
        "        y_ns = sched['no_show'].astype(int)\n",
        "        X_ns = sched[feature_cols]\n",
        "        pre = ColumnTransformer([\n",
        "            ('ohe', OneHotEncoder(handle_unknown='ignore'), feature_cols)\n",
        "        ])\n",
        "        noshow_model = Pipeline([\n",
        "            ('pre', pre),\n",
        "            ('clf', LogisticRegression(max_iter=1000))\n",
        "        ])\n",
        "        noshow_model.fit(X_ns, y_ns)\n",
        "        p_ns = float(y_ns.mean())\n",
        "        calib['no_show_prob'] = p_ns\n",
        "\n",
        "        # Late model (scheduled only, arrived)\n",
        "        sched_arrived = sched[sched['no_show'] == 0].copy()\n",
        "        y_late = sched_arrived['late'].astype(int)\n",
        "        X_lt = sched_arrived[feature_cols]\n",
        "        late_model = Pipeline([\n",
        "            ('pre', pre),\n",
        "            ('clf', LogisticRegression(max_iter=1000))\n",
        "        ])\n",
        "        late_model.fit(X_lt, y_late)\n",
        "        p_lt = float(y_late.mean())\n",
        "        calib['late_prob'] = p_lt\n",
        "\n",
        "        print('Fitted no-show baseline prob:', round(p_ns, 3), '| late baseline prob:', round(p_lt, 3))\n",
        "    else:\n",
        "        print('ARRIVALS_CSV not set; skipping model fit.')\n",
        "except Exception as e:\n",
        "    print('Model fitting failed; using default calibration. Reason:', e)\n",
        "\n",
        "# Set domain randomization ranges around calibrated values\n",
        "RAND_RANGES = {\n",
        "    'walkin_rate_per_hour': (max(2.0, calib['walkin_rate_per_hour'] * 0.7), calib['walkin_rate_per_hour'] * 1.4),\n",
        "    'no_show_prob': (max(0.0, calib['no_show_prob'] * 0.7), min(0.5, calib['no_show_prob'] * 1.4)),\n",
        "    'late_prob': (max(0.0, calib['late_prob'] * 0.7), min(0.6, calib['late_prob'] * 1.4)),\n",
        "    'service_mean_min': (max(4.0, calib['service_mean_min'] * 0.8), calib['service_mean_min'] * 1.2),\n",
        "    'service_sigma_min': (max(0.5, calib['service_sigma_min'] * 0.8), calib['service_sigma_min'] * 1.3),\n",
        "}\n",
        "print('Updated RAND_RANGES:', RAND_RANGES)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0a5e4834"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Short Optuna tuning on calibrated env (adjust n_trials for compute)\n",
        "try:\n",
        "    import optuna\n",
        "    HAS_OPTUNA = True\n",
        "except Exception:\n",
        "    HAS_OPTUNA = False\n",
        "\n",
        "best_params = None\n",
        "\n",
        "if HAS_OPTUNA:\n",
        "    def make_env_for_tuning():\n",
        "        # use calibrated params\n",
        "        return ClinicSchedulingEnvV2(\n",
        "            slot_minutes=calib['slot_minutes'],\n",
        "            num_providers=calib['num_providers'],\n",
        "            service_mean_min=calib['service_mean_min'],\n",
        "            service_sigma_min=calib['service_sigma_min'],\n",
        "            walkin_rate_per_hour=calib['walkin_rate_per_hour'],\n",
        "            no_show_prob=calib['no_show_prob'],\n",
        "            late_prob=calib['late_prob'],\n",
        "            walkin_cutoff_minute=calib['walkin_cutoff_minute'],\n",
        "            seeded_schedule=seed_schedule,\n",
        "        )\n",
        "\n",
        "    def objective(trial):\n",
        "        lr = trial.suggest_float('lr', 1e-5, 5e-4, log=True)\n",
        "        n_steps = trial.suggest_int('n_steps', 512, 4096, log=True)\n",
        "        batch_size = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
        "        gamma = trial.suggest_float('gamma', 0.98, 0.999)\n",
        "        from stable_baselines3 import PPO\n",
        "        env = make_env_for_tuning()\n",
        "        model = PPO('MlpPolicy', env, verbose=0, learning_rate=lr, n_steps=n_steps, batch_size=batch_size, gamma=gamma)\n",
        "        model.learn(total_timesteps=50_000)\n",
        "        # quick metric: scheduled served\n",
        "        e = make_env_for_tuning()\n",
        "        obs, _ = e.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a, _ = model.predict(obs, deterministic=True)\n",
        "            obs, r, term, trunc, _ = e.step(int(a))\n",
        "            done = bool(term or trunc)\n",
        "        return len([1 for row in e.served_log if not row['is_walkin']])\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=10)\n",
        "    best_params = study.best_params\n",
        "    print('Tuned hyperparameters:', best_params)\n",
        "else:\n",
        "    print('Optuna not installed; skipping hyperparameter tuning.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c7660a97"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibrated training setup (Drive mount + env + training)\n",
        "This cell mounts Google Drive, sets file paths, redefines the environment factory to use calibrated parameters (`calib`), and runs calibrated training with MaskablePPO if available."
      ],
      "id": "e87c8b82"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mount Drive and set base dir\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_DIR = '/content/drive/MyDrive/clinic_scheduling_rl'\n",
        "except Exception:\n",
        "    import os\n",
        "    BASE_DIR = '/workspace/drive'\n",
        "\n",
        "import os\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# Optional: set CSV paths here if not set earlier\n",
        "ARRIVALS_CSV = ARRIVALS_CSV or None\n",
        "SERVICE_CSV  = SERVICE_CSV  or None\n",
        "\n",
        "# Redefine env factory to use calibrated params\n",
        "\n",
        "def make_env_v2():\n",
        "    return ClinicSchedulingEnvV2(\n",
        "        slot_minutes=calib[\"slot_minutes\"],\n",
        "        num_providers=calib[\"num_providers\"],\n",
        "        service_mean_min=calib[\"service_mean_min\"],\n",
        "        service_sigma_min=calib[\"service_sigma_min\"],\n",
        "        walkin_rate_per_hour=calib[\"walkin_rate_per_hour\"],\n",
        "        no_show_prob=calib[\"no_show_prob\"],\n",
        "        late_prob=calib[\"late_prob\"],\n",
        "        walkin_cutoff_minute=calib[\"walkin_cutoff_minute\"],\n",
        "        seeded_schedule=seed_schedule,\n",
        "    )\n",
        "\n",
        "# Calibrated training\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "vec_env_v2 = make_vec_env(make_env_v2, n_envs=1)\n",
        "\n",
        "try:\n",
        "    from sb3_contrib import MaskablePPO\n",
        "    hp = best_params or {'lr':3e-4,'n_steps':2048,'batch_size':256,'gamma':0.995}\n",
        "    model_v2 = MaskablePPO(\"MlpPolicy\", vec_env_v2, verbose=1,\n",
        "                           learning_rate=hp['lr'], n_steps=hp['n_steps'],\n",
        "                           batch_size=hp['batch_size'], gamma=hp['gamma'])\n",
        "except Exception:\n",
        "    from stable_baselines3 import PPO\n",
        "    hp = best_params or {'lr':3e-4,'n_steps':2048,'batch_size':256,'gamma':0.995}\n",
        "    model_v2 = PPO(\"MlpPolicy\", vec_env_v2, verbose=1,\n",
        "                   learning_rate=hp['lr'], n_steps=hp['n_steps'],\n",
        "                   batch_size=hp['batch_size'], gamma=hp['gamma'])\n",
        "\n",
        "model_v2.learn(total_timesteps=500_000)\n",
        "\n",
        "# Save to Drive\n",
        "model_v2_path = os.path.join(BASE_DIR, 'model_v2.zip')\n",
        "model_v2.save(model_v2_path)\n",
        "print('Saved calibrated model to', model_v2_path)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7eea055d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Richer observation wrapper, Behavior Cloning (BC) pretrain, RCPO, and robust training\n",
        "Adds:\n",
        "- ObservationAugmentWrapper: time-to-lunch/close, provider load stats, near-term backlog, predicted no-show/late probs\n",
        "- BC pretrain on heuristic rollouts\n",
        "- RCPO-style constrained training (SLA≤30 min)\n",
        "- VecNormalize + multiple envs training\n",
        "- 100-episode confidence interval evaluation"
      ],
      "id": "77ec0c7b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Richer observation augmentation\n",
        "class ObservationAugmentWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env: ClinicSchedulingEnvV2, noshow_model=None, late_model=None):\n",
        "        super().__init__(env)\n",
        "        self.env: ClinicSchedulingEnvV2\n",
        "        self.noshow_model = noshow_model\n",
        "        self.late_model = late_model\n",
        "        orig = env.observation_space.shape[0]\n",
        "        # extra features: time_to_lunch, time_to_close, provider_busy_frac, backlog_next_30min, p_no_show_next, p_late_next\n",
        "        self.extra_dim = 6\n",
        "        high = np.concatenate([\n",
        "            env.observation_space.high,\n",
        "            np.array([300, 600, 1.0, 50, 1.0, 1.0], dtype=np.float32)\n",
        "        ])\n",
        "        self.observation_space = spaces.Box(low=0.0, high=high, dtype=np.float32)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        minute = self.env.minute\n",
        "        ttl = max(0, MINUTES_LUNCH_START - minute) if minute < MINUTES_LUNCH_START else max(0, MINUTES_CLOSE_PM - minute)\n",
        "        ttc = max(0, MINUTES_CLOSE_PM - minute)\n",
        "        busy = sum(1 for t in self.env.provider_busy_remaining if t > 0)\n",
        "        busy_frac = busy / max(1, self.env.num_providers)\n",
        "        # approximate backlog in next 30 min (scheduled + walk-ins)\n",
        "        near_sched = sum(\n",
        "            1\n",
        "            for slot, patient in self.env._remaining_scheduled()\n",
        "            if self.env._slot_to_minute(slot) < minute + 30\n",
        "        )\n",
        "        near_backlog = near_sched + len(self.env.walkin_queue)\n",
        "        # simple next scheduled probs\n",
        "        p_ns = 0.0; p_lt = 0.0\n",
        "        try:\n",
        "            next_slot, _ = next(self.env._remaining_scheduled(), (None, None))\n",
        "            if next_slot is not None:\n",
        "                booked_minute = self.env._slot_to_minute(next_slot)\n",
        "                df_feat = pd.DataFrame([{\n",
        "                    'hour': booked_minute // 60,\n",
        "                    'dow': 0\n",
        "                }])\n",
        "                if self.noshow_model is not None:\n",
        "                    p_ns = float(self.noshow_model.predict_proba(df_feat)[0,1])\n",
        "                if self.late_model is not None:\n",
        "                    p_lt = float(self.late_model.predict_proba(df_feat)[0,1])\n",
        "        except Exception:\n",
        "            pass\n",
        "        extra = np.array([ttl, ttc, busy_frac, near_backlog, p_ns, p_lt], dtype=np.float32)\n",
        "        return np.concatenate([obs, extra], axis=0)\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "924a552b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Offline Replay (Historical Days)\n",
        "Provide one JSON/CSV path per clinic day with the sequence of scheduled arrivals, walk-ins, and staff actions.\n",
        "Configure the list below, then run the replay cell to compare heuristic vs. RL policies on actual demand."
      ],
      "id": "45612a28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Offline replay driver (CSV or JSON day files)\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "HISTORICAL_DAY_FILES = []  # e.g., [\"/content/drive/MyDrive/clinic_logs/days/2025-01-05.csv\", ...]\n",
        "\n",
        "\n",
        "class HistoricalClinicEnv(ClinicSchedulingEnvV2):\n",
        "    \"\"\"Environment that replays observed schedules/walk-ins instead of sampling.\"\"\"\n",
        "    def __init__(self, day_data, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._scheduled_plan = day_data.get('scheduled', [])\n",
        "        self._walkin_queue_plan = day_data.get('walkins', [])\n",
        "        self._action_log = day_data.get('actions', [])\n",
        "        self._action_idx = 0\n",
        "        self._build_scheduled_from_observed()\n",
        "\n",
        "    def _build_scheduled_from_observed(self):\n",
        "        self.scheduled_slots.clear()\n",
        "        pid_max = 0\n",
        "        for entry in self._scheduled_plan:\n",
        "            slot = int(entry['slot'])\n",
        "            pid = entry['patient_id']\n",
        "            arrival = entry.get('arrival_minute')\n",
        "            if arrival == '' or arrival is None:\n",
        "                arrival = None\n",
        "            else:\n",
        "                arrival = int(arrival)\n",
        "            patient = Patient(id=pid, scheduled_slot=slot, arrival_time_min=arrival)\n",
        "            patient.is_late = bool(int(entry.get('late', 0))) if entry.get('late') not in (None, '') else False\n",
        "            self.scheduled_slots[slot].append(patient)\n",
        "            self.generated_patients[pid] = patient\n",
        "            pid_max = max(pid_max, pid)\n",
        "        self.next_walkin_id = pid_max + 1\n",
        "\n",
        "    def _maybe_generate_walkins(self):\n",
        "        while self._walkin_queue_plan and int(self._walkin_queue_plan[0]['minute']) <= self.minute:\n",
        "            entry = self._walkin_queue_plan.pop(0)\n",
        "            pid = entry.get('patient_id')\n",
        "            if pid in (None, ''):\n",
        "                pid = self.next_walkin_id\n",
        "                self.next_walkin_id += 1\n",
        "            minute = int(entry['minute'])\n",
        "            patient = Patient(id=pid, scheduled_slot=None, arrival_time_min=minute)\n",
        "            self.walkin_queue.append(patient)\n",
        "            self.generated_patients[pid] = patient\n",
        "\n",
        "    def step(self, action):\n",
        "        if self._action_log:\n",
        "            action = int(self._action_log[min(self._action_idx, len(self._action_log)-1)])\n",
        "            self._action_idx += 1\n",
        "        return super().step(action)\n",
        "\n",
        "\n",
        "def _load_day_file(day_path):\n",
        "    day_path = Path(day_path)\n",
        "    if day_path.suffix.lower() == '.json':\n",
        "        data = json.loads(day_path.read_text())\n",
        "    else:\n",
        "        import pandas as pd\n",
        "        df = pd.read_csv(day_path)\n",
        "        if 'type' not in df.columns:\n",
        "            raise ValueError('Historical CSV must include a \"type\" column (scheduled/walkin/action).')\n",
        "        data = {\n",
        "            'scheduled': df[df['type'].str.lower() == 'scheduled'].to_dict('records'),\n",
        "            'walkins': df[df['type'].str.lower() == 'walkin'].to_dict('records'),\n",
        "            'actions': df[df['type'].str.lower() == 'action']['action'].dropna().tolist()\n",
        "        }\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_historical_env(day_path, calib_overrides=None):\n",
        "    data = _load_day_file(day_path)\n",
        "    overrides = calib.copy()\n",
        "    if calib_overrides:\n",
        "        overrides.update(calib_overrides)\n",
        "    env = HistoricalClinicEnv(\n",
        "        data,\n",
        "        slot_minutes=overrides['slot_minutes'],\n",
        "        num_providers=overrides['num_providers'],\n",
        "        service_mean_min=overrides['service_mean_min'],\n",
        "        service_sigma_min=overrides['service_sigma_min'],\n",
        "        walkin_rate_per_hour=overrides['walkin_rate_per_hour'],\n",
        "        no_show_prob=overrides['no_show_prob'],\n",
        "        late_prob=overrides['late_prob'],\n",
        "        walkin_cutoff_minute=overrides['walkin_cutoff_minute'],\n",
        "    )\n",
        "    return env\n",
        "\n",
        "\n",
        "def evaluate_historical_days(day_files, policy_factories=None):\n",
        "    if not day_files:\n",
        "        print('HISTORICAL_DAY_FILES is empty; add paths to replay.')\n",
        "        return None\n",
        "    if policy_factories is None:\n",
        "        policy_factories = {\n",
        "            'Heuristic': lambda env: HeuristicPolicy(),\n",
        "            'Safe RL (fresh)': lambda env: SafePolicy(env, model_robust)\n",
        "        }\n",
        "    all_metrics = {}\n",
        "    for policy_name, factory in policy_factories.items():\n",
        "        rows = []\n",
        "        for path in day_files:\n",
        "            env = load_historical_env(path)\n",
        "            policy = factory(env)\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                if hasattr(policy, 'predict'):\n",
        "                    action, _ = policy.predict(obs, deterministic=True)\n",
        "                else:\n",
        "                    action = policy.select_action(env)\n",
        "                obs, reward, term, trunc, _ = env.step(int(action))\n",
        "                done = bool(term or trunc)\n",
        "            df = pd.DataFrame(env.served_log)\n",
        "            rows.append(episode_metrics(env, df))\n",
        "        all_metrics[policy_name] = pd.DataFrame(rows)\n",
        "        print(f\"=== Offline replay: {policy_name} ===\")\n",
        "        print(all_metrics[policy_name].describe().loc[['mean','std','min','max']])\n",
        "        print()\n",
        "    return all_metrics\n",
        "\n",
        "# Example run (set HISTORICAL_DAY_FILES first):\n",
        "# historical_results = evaluate_historical_days(HISTORICAL_DAY_FILES)\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "352e2f59"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run offline replay (execute after populating HISTORICAL_DAY_FILES)\n",
        "if HISTORICAL_DAY_FILES:\n",
        "    historical_results = evaluate_historical_days(HISTORICAL_DAY_FILES)\n",
        "    print('Replay completed for', len(HISTORICAL_DAY_FILES), 'day(s).')\n",
        "else:\n",
        "    print('HISTORICAL_DAY_FILES is empty; set file paths before running the replay.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "70cc6008"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Behavior Cloning (BC) pretrain on heuristic rollouts\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class BCPolicy(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim=3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, act_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def collect_heuristic_dataset(n_episodes=50):\n",
        "    ds_obs, ds_act = [], []\n",
        "    for _ in range(n_episodes):\n",
        "        env = make_env_v2()\n",
        "        env = ObservationAugmentWrapper(env, noshow_model, late_model)\n",
        "        policy = HeuristicPolicy()\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy.select_action(env)\n",
        "            ds_obs.append(obs.copy())\n",
        "            ds_act.append(action)\n",
        "            obs, r, term, trunc, _ = env.step(action)\n",
        "            done = bool(term or trunc)\n",
        "    X = torch.tensor(np.array(ds_obs), dtype=torch.float32)\n",
        "    y = torch.tensor(np.array(ds_act), dtype=torch.long)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def bc_pretrain(epochs=5, batch=256):\n",
        "    env_tmp = ObservationAugmentWrapper(make_env_v2(), noshow_model, late_model)\n",
        "    obs_dim = env_tmp.observation_space.shape[0]\n",
        "    model = BCPolicy(obs_dim)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    X, y = collect_heuristic_dataset(n_episodes=50)\n",
        "    for ep in range(epochs):\n",
        "        idx = torch.randperm(X.size(0))\n",
        "        for i in range(0, X.size(0), batch):\n",
        "            b = idx[i:i+batch]\n",
        "            logits = model(X[b])\n",
        "            loss = loss_fn(logits, y[b])\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "        print(f\"BC epoch {ep+1}/{epochs} loss {loss.item():.4f}\")\n",
        "    return model\n",
        "\n",
        "# Run BC\n",
        "bc_model = bc_pretrain(epochs=5)\n",
        "print(\"BC pretrain complete\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0545fe39"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deployment Readiness Validation Harness\n",
        "Use this section to run offline acceptance tests before shadowing or deploying: compare heuristic vs. RL, verify SLA compliance, and ensure hard constraints (no lunch service, no leftover queues) are respected."
      ],
      "id": "33abdb97"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluation harness for policy readiness\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "def make_base_eval_env(randomized: bool = False):\n",
        "    if randomized and 'rand_factory' in globals():\n",
        "        return rand_factory.make_env()\n",
        "    return make_env_v2()\n",
        "\n",
        "\n",
        "def unwrap_to_base(env):\n",
        "    current = env\n",
        "    while hasattr(current, 'env'):\n",
        "        current = current.env\n",
        "    if hasattr(current, 'unwrapped'):\n",
        "        return current.unwrapped\n",
        "    return current\n",
        "\n",
        "\n",
        "def episode_metrics(base_env, df: pd.DataFrame):\n",
        "    metrics = {}\n",
        "    metrics['served_total'] = len(df)\n",
        "    metrics['avg_wait'] = float(df['wait_minutes'].dropna().mean()) if not df.empty else float('nan')\n",
        "    metrics['p95_wait'] = float(df['wait_minutes'].dropna().quantile(0.95)) if not df.empty else float('nan')\n",
        "    metrics['sla_compliance'] = float((df['wait_minutes'].fillna(999) <= 30).mean()) if not df.empty else float('nan')\n",
        "    metrics['scheduled_unserved'] = int(sum(1 for _ in base_env._remaining_scheduled()))\n",
        "    metrics['walkins_remaining'] = int(len(base_env.walkin_queue))\n",
        "    metrics['late_remaining'] = int(sum(1 for p in base_env.late_list if p.arrival_time_min is not None))\n",
        "\n",
        "    alerts = []\n",
        "    if not df.empty and df['served_minute'].between(MINUTES_LUNCH_START, MINUTES_LUNCH_END - 1).any():\n",
        "        alerts.append('served_during_lunch')\n",
        "    if base_env.minute > MINUTES_CLOSE_PM:\n",
        "        alerts.append(f'closed_overrun={base_env.minute - MINUTES_CLOSE_PM}')\n",
        "    if metrics['scheduled_unserved']:\n",
        "        alerts.append(f'unserved_scheduled={metrics[\"scheduled_unserved\"]}')\n",
        "    if metrics['walkins_remaining']:\n",
        "        alerts.append(f'walkins_remaining={metrics[\"walkins_remaining\"]}')\n",
        "    if metrics['late_remaining']:\n",
        "        alerts.append(f'late_remaining={metrics[\"late_remaining\"]}')\n",
        "    metrics['alerts'] = alerts\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def ci95(series: pd.Series):\n",
        "    x = series.dropna().values\n",
        "    if len(x) < 2:\n",
        "        return (float('nan'), float('nan'))\n",
        "    mean = float(np.mean(x))\n",
        "    se = float(np.std(x, ddof=1) / np.sqrt(len(x)))\n",
        "    return (mean - 1.96 * se, mean + 1.96 * se)\n",
        "\n",
        "\n",
        "def evaluate_policy_suite(policy_factories: dict, episodes: int = 30, randomized: bool = False, seed: int | None = None):\n",
        "    results = {}\n",
        "    for name, factory in policy_factories.items():\n",
        "        metrics = []\n",
        "        for ep in range(episodes):\n",
        "            env = make_base_eval_env(randomized=randomized)\n",
        "            policy = factory(env)\n",
        "            obs, _ = env.reset(seed=seed + ep if seed is not None else None)\n",
        "            done = False\n",
        "            while not done:\n",
        "                if hasattr(policy, 'predict'):\n",
        "                    action, _ = policy.predict(obs, deterministic=True)\n",
        "                else:\n",
        "                    action = policy.select_action(env)\n",
        "                obs, reward, term, trunc, _ = env.step(int(action))\n",
        "                done = bool(term or trunc)\n",
        "\n",
        "            base_env = unwrap_to_base(env)\n",
        "            df = pd.DataFrame(base_env.served_log)\n",
        "            metrics.append(episode_metrics(base_env, df))\n",
        "\n",
        "        df_metrics = pd.DataFrame(metrics)\n",
        "        results[name] = df_metrics\n",
        "\n",
        "        print(f\"=== {name} ({'randomized' if randomized else 'calibrated'} env) ===\")\n",
        "        summary_cols = ['served_total', 'avg_wait', 'p95_wait', 'sla_compliance']\n",
        "        print(df_metrics[summary_cols].describe().loc[['mean', 'std', 'min', 'max']])\n",
        "        for col in summary_cols:\n",
        "            print(f\"CI {col}: {ci95(df_metrics[col])}\")\n",
        "        alert_count = df_metrics['alerts'].apply(bool).sum()\n",
        "        if alert_count:\n",
        "            print(f\"Alerts triggered in {alert_count} / {episodes} episodes\")\n",
        "            print(df_metrics.loc[df_metrics['alerts'].apply(bool), 'alerts'].head())\n",
        "        else:\n",
        "            print('No safety alerts triggered.')\n",
        "        print()\n",
        "    return results\n",
        "\n",
        "\n",
        "policy_factories = {'Heuristic': lambda env: HeuristicPolicy()}\n",
        "if 'loaded_v2' in globals():\n",
        "    policy_factories['Safe RL (loaded_v2)'] = lambda env: SafePolicy(env, loaded_v2)\n",
        "\n",
        "_ = evaluate_policy_suite(policy_factories, episodes=30, randomized=False)\n",
        "if 'rand_factory' in globals():\n",
        "    _ = evaluate_policy_suite(policy_factories, episodes=30, randomized=True)\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0b843ed3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# RCPO-style constrained RL (SLA <= 30 min)\n",
        "class RCPOWrapper(gym.Wrapper):\n",
        "    def __init__(self, env: ClinicSchedulingEnvV2, sla_minutes=30, lam=0.01, lam_lr=1e-4):\n",
        "        super().__init__(env)\n",
        "        self.sla = sla_minutes\n",
        "        self.lam = lam\n",
        "        self.lam_lr = lam_lr\n",
        "        self.violations = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        base_env = self.env.unwrapped if hasattr(self.env, \"unwrapped\") else self.env\n",
        "        served_log = getattr(base_env, \"served_log\", None)\n",
        "        prev_len = len(served_log) if served_log is not None else 0\n",
        "\n",
        "        obs, reward, done, trunc, info = self.env.step(action)\n",
        "\n",
        "        served_log = getattr(base_env, \"served_log\", None)\n",
        "        if served_log:\n",
        "            new_entries = served_log[prev_len:]\n",
        "            for entry in new_entries:\n",
        "                w = entry.get(\"wait_minutes\")\n",
        "                if w is not None and w > self.sla:\n",
        "                    self.violations += 1\n",
        "                    reward -= self.lam\n",
        "\n",
        "        if done:\n",
        "            total = max(1, len(served_log) if served_log is not None else 1)\n",
        "            target = 0.15  # target 15% or less over-SLA\n",
        "            rate = self.violations / total\n",
        "            self.lam += self.lam_lr * (rate - target)\n",
        "            self.lam = max(0.0, min(1.0, self.lam))\n",
        "            self.violations = 0\n",
        "\n",
        "        return obs, reward, done, trunc, info\n",
        "\n",
        "\n",
        "def make_augmented_env():\n",
        "    base = make_env_v2()\n",
        "    aug = ObservationAugmentWrapper(base, noshow_model, late_model)\n",
        "    rcpo = RCPOWrapper(aug)\n",
        "    return rcpo"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fca802b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Robust training: VecNormalize + multiple envs\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "n_envs = 4\n",
        "vec_env = DummyVecEnv([make_augmented_env for _ in range(n_envs)])\n",
        "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)\n",
        "\n",
        "model_robust = PPO('MlpPolicy', vec_env, verbose=1, n_steps=2048, batch_size=512, gamma=0.995, learning_rate=3e-4)\n",
        "model_robust.learn(total_timesteps=1_000_000)\n",
        "print('Robust training complete')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f59c4500"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 100-episode CI evaluation (aligned with augmented + normalized training env)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "def ci95(arr):\n",
        "    x = np.array(arr, dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    if len(x) < 2:\n",
        "        return (float('nan'), float('nan'))\n",
        "    m = float(np.mean(x))\n",
        "    se = float(np.std(x, ddof=1) / sqrt(len(x)))\n",
        "    return (m - 1.96 * se, m + 1.96 * se)\n",
        "\n",
        "\n",
        "def make_eval_env(model):\n",
        "    base = DummyVecEnv([make_augmented_env])\n",
        "    eval_env = VecNormalize(base, training=False, norm_obs=True, norm_reward=False)\n",
        "    training_env = model.get_env()\n",
        "    if training_env is not None and hasattr(training_env, 'obs_rms'):\n",
        "        eval_env.obs_rms = training_env.obs_rms.copy()\n",
        "        eval_env.clip_obs = getattr(training_env, 'clip_obs', eval_env.clip_obs)\n",
        "    return eval_env\n",
        "\n",
        "\n",
        "def unwrap_env(env):\n",
        "    current = env\n",
        "    while hasattr(current, 'env'):\n",
        "        current = current.env\n",
        "    if hasattr(current, 'unwrapped'):\n",
        "        return current.unwrapped\n",
        "    return current\n",
        "\n",
        "\n",
        "def evaluate_model(model, episodes=100):\n",
        "    env = make_eval_env(model)\n",
        "    served_totals, avg_waits, sla_compliance, p95_waits, violations = [], [], [], [], []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done_vec, info = env.step(action)\n",
        "            done = bool(done_vec[0])\n",
        "\n",
        "        base_env = unwrap_env(env.envs[0])\n",
        "        df = pd.DataFrame(base_env.served_log)\n",
        "        served_totals.append(len(df))\n",
        "        avg_waits.append(float(df['wait_minutes'].dropna().mean()) if not df.empty else float('nan'))\n",
        "        p95_waits.append(float(df['wait_minutes'].dropna().quantile(0.95)) if not df.empty else float('nan'))\n",
        "        sla_compliance.append(float((df['wait_minutes'].fillna(999) <= 30).mean()) if not df.empty else float('nan'))\n",
        "\n",
        "        alerts = []\n",
        "        if not df.empty and (df['served_minute'].between(MINUTES_LUNCH_START, MINUTES_LUNCH_END - 1).any()):\n",
        "            alerts.append('served_during_lunch')\n",
        "        remaining_sched = sum(1 for _ in base_env._remaining_scheduled())\n",
        "        if remaining_sched:\n",
        "            alerts.append(f'unserved_scheduled={remaining_sched}')\n",
        "        if len(base_env.walkin_queue):\n",
        "            alerts.append(f'walkins_remaining={len(base_env.walkin_queue)}')\n",
        "        violations.append(alerts)\n",
        "\n",
        "    def summarize(name, data):\n",
        "        print(f\"{name}: mean={np.nanmean(data):.2f}, CI={ci95(data)}\")\n",
        "\n",
        "    print('=== Robust PPO evaluation (augmented + normalized env) ===')\n",
        "    summarize('Served total', served_totals)\n",
        "    summarize('Average wait', avg_waits)\n",
        "    summarize('95th percentile wait', p95_waits)\n",
        "    summarize('SLA<=30min compliance', sla_compliance)\n",
        "    flagged = [v for v in violations if v]\n",
        "    if flagged:\n",
        "        print(f\"Alerts triggered in {len(flagged)} / {episodes} episodes -> {flagged[:3]}\")\n",
        "    else:\n",
        "        print('No safety alerts triggered across evaluated episodes.')\n",
        "\n",
        "\n",
        "evaluate_model(model_robust, episodes=100)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8005d406"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## End-to-End Training Runner\n",
        "This cell installs any missing dependencies, trains the robust PPO policy with observation augmentation, behavior-cloned warm-start, and RCPO constraints, then runs the evaluation harnesses defined above.\n",
        "\n",
        "- Call `train_end_to_end(total_timesteps=300_000, eval_randomized=True)` to kick off training + evaluations.\n",
        "- Adjust `total_timesteps`, `n_envs`, or `log_dir` as needed; artifacts and VecNormalize stats write to the chosen log directory.\n",
        "- The function automatically runs the calibrated/domain-randomized evaluation suites and the 100-episode CI check when those helpers are defined."
      ],
      "id": "fb0cf4db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# End-to-end training orchestrator (install deps, train, evaluate)\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import pathlib\n",
        "import importlib\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def _ensure_rl_dependencies():\n",
        "    required = [\n",
        "        (\"stable_baselines3\", \"stable-baselines3==2.3.2\"),\n",
        "        (\"sb3_contrib\", \"sb3-contrib==2.3.2\"),\n",
        "        (\"shimmy\", \"shimmy==1.3.0\"),\n",
        "        (\"gymnasium\", \"gymnasium==0.29.1\"),\n",
        "    ]\n",
        "    missing = [pkg for pkg, pip_name in required if importlib.util.find_spec(pkg) is None]\n",
        "    if not missing:\n",
        "        return\n",
        "    print(\"Installing missing RL dependencies:\", missing)\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + [pip_name for pkg, pip_name in required if pkg in missing]\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "\n",
        "def train_end_to_end(total_timesteps: int = 200_000,\n",
        "                     n_envs: int = 4,\n",
        "                     log_dir: str = None,\n",
        "                     eval_episodes: int = 30,\n",
        "                     eval_randomized: bool = False,\n",
        "                     save_name: str = \"model_robust\"):\n",
        "    \"\"\"Full pipeline: dependency check, training on augmented+RCPO env, evaluation.\"\"\"\n",
        "    _ensure_rl_dependencies()\n",
        "    from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "    try:\n",
        "        from sb3_contrib import MaskablePPO as Algo\n",
        "        algo_name = \"MaskablePPO\"\n",
        "        algo_kwargs = {}\n",
        "    except Exception:\n",
        "        from stable_baselines3 import PPO as Algo\n",
        "        algo_name = \"PPO\"\n",
        "        algo_kwargs = {}\n",
        "\n",
        "    if log_dir is None:\n",
        "        base_dir = pathlib.Path(\"/content\") if pathlib.Path(\"/content\").exists() else pathlib.Path.cwd()\n",
        "        log_dir = base_dir / \"logs_end_to_end\"\n",
        "    log_dir = pathlib.Path(log_dir)\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Using algorithm: {algo_name} | total_timesteps={total_timesteps:,} | n_envs={n_envs}\")\n",
        "\n",
        "    vec_env = DummyVecEnv([make_augmented_env for _ in range(n_envs)])\n",
        "    vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
        "\n",
        "    model = Algo(\n",
        "        \"MlpPolicy\",\n",
        "        vec_env,\n",
        "        verbose=1,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=512,\n",
        "        gamma=0.995,\n",
        "        gae_lambda=0.95,\n",
        "        **algo_kwargs,\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    globals()['model_robust'] = model\n",
        "\n",
        "    model_path = log_dir / f\"{save_name}.zip\"\n",
        "    model.save(model_path)\n",
        "    print(f\"Saved policy to {model_path}\")\n",
        "\n",
        "    try:\n",
        "        vec_env.save(log_dir / \"vecnormalize.pkl\")\n",
        "        print(f\"Saved VecNormalize statistics to {log_dir / 'vecnormalize.pkl'}\")\n",
        "    except Exception as exc:\n",
        "        print(\"Warning: could not save VecNormalize stats:\", exc)\n",
        "\n",
        "    if 'evaluate_policy_suite' in globals():\n",
        "        factories = {'Heuristic': lambda env: HeuristicPolicy()}\n",
        "        if 'loaded_v2' in globals():\n",
        "            factories['Safe RL (loaded_v2)'] = lambda env: SafePolicy(env, loaded_v2)\n",
        "        factories['Safe RL (fresh)'] = lambda env: SafePolicy(env, model)\n",
        "        print(\"\\nRunning calibrated evaluation suite ...\")\n",
        "        evaluate_policy_suite(factories, episodes=eval_episodes, randomized=False)\n",
        "        if eval_randomized and 'rand_factory' in globals():\n",
        "            print(\"\\nRunning domain-randomized evaluation suite ...\")\n",
        "            evaluate_policy_suite(factories, episodes=eval_episodes, randomized=True)\n",
        "\n",
        "    if 'evaluate_model' in globals():\n",
        "        print(\"\\nRunning 100-episode CI evaluation ...\")\n",
        "        evaluate_model(model, episodes=max(20, eval_episodes))\n",
        "    else:\n",
        "        print(\"Note: evaluate_model() not defined yet; skip 100-episode CI run.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Example usage (commented to avoid accidental long runs):\n",
        "# trained_model = train_end_to_end(total_timesteps=300_000, n_envs=4, eval_episodes=20, eval_randomized=True)\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "39f67692"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d454cfb"
      },
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "SLOT_MINUTES_BOOKING = 7\n",
        "OPEN_HOURS = [8, 9, 10, 11, 13, 14, 15]\n",
        "\n",
        "\n",
        "def _capacity_for_hour(hour: int, slot_minutes: int = SLOT_MINUTES_BOOKING) -> int:\n",
        "    # Hours just before a closed boundary (12:00 lunch, 16:00 close) cannot bleed over\n",
        "    if hour == 11:\n",
        "        return (60) // slot_minutes  # end by 12:00\n",
        "    if hour == 15:\n",
        "        return (60) // slot_minutes  # end by 16:00\n",
        "    # Other hours may bleed into next hour a bit\n",
        "    from math import ceil\n",
        "    return int(ceil(60.0 / slot_minutes))\n",
        "\n",
        "\n",
        "def _minute_to_str(minute_of_day: int) -> str:\n",
        "    h = minute_of_day // 60\n",
        "    m = minute_of_day % 60\n",
        "    suffix = \"am\" if h < 12 else \"pm\"\n",
        "    h12 = h if 1 <= h <= 12 else (h - 12 if h > 12 else 12)\n",
        "    return f\"{h12}:{m:02d}{suffix}\"\n",
        "\n",
        "\n",
        "def _format_hour_ranges(hours: List[int]) -> str:\n",
        "    if not hours:\n",
        "        return \"(none)\"\n",
        "    hours = sorted(hours)\n",
        "    ranges: List[Tuple[int, int]] = []  # [start, end_exclusive]\n",
        "    start = hours[0]\n",
        "    prev = hours[0]\n",
        "    for h in hours[1:]:\n",
        "        if h == prev + 1 or (prev == 11 and h == 13):\n",
        "            # treat lunch gap as break; so 11->13 is not consecutive\n",
        "            if prev == 11 and h == 13:\n",
        "                ranges.append((start, prev + 1))\n",
        "                start = h\n",
        "            # else keep extending\n",
        "        else:\n",
        "            ranges.append((start, prev + 1))\n",
        "            start = h\n",
        "        prev = h\n",
        "    ranges.append((start, prev + 1))\n",
        "\n",
        "    def to_12h(h):\n",
        "        suffix = \"am\" if h < 12 else \"pm\"\n",
        "        h12 = h if 1 <= h <= 12 else (h - 12 if h > 12 else 12)\n",
        "        return f\"{h12}{suffix}\"\n",
        "\n",
        "    return \", \".join([f\"{to_12h(s)}–{to_12h(e)}\" for s, e in ranges])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BookingPlanner:\n",
        "    slot_minutes: int = SLOT_MINUTES_BOOKING\n",
        "    open_hours: List[int] = field(default_factory=lambda: OPEN_HOURS.copy())\n",
        "    capacity_by_hour: Dict[int, int] = field(init=False)\n",
        "    booked_count_by_hour: Dict[int, int] = field(init=False)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.capacity_by_hour = {h: _capacity_for_hour(h, self.slot_minutes) for h in self.open_hours}\n",
        "        self.booked_count_by_hour = {h: 0 for h in self.open_hours}\n",
        "\n",
        "    def available_hours(self) -> List[int]:\n",
        "        return [h for h in self.open_hours if self.booked_count_by_hour[h] < self.capacity_by_hour[h]]\n",
        "\n",
        "    def book(self, hour: int) -> Optional[str]:\n",
        "        if hour not in self.open_hours:\n",
        "            return None\n",
        "        cap = self.capacity_by_hour[hour]\n",
        "        used = self.booked_count_by_hour[hour]\n",
        "        if used >= cap:\n",
        "            return None\n",
        "        # Assign next 7-min slot within the hour\n",
        "        start_minute = hour * 60 + used * self.slot_minutes\n",
        "        self.booked_count_by_hour[hour] += 1\n",
        "        return _minute_to_str(start_minute)\n",
        "\n",
        "    def availability_label(self) -> str:\n",
        "        return _format_hour_ranges(self.available_hours())\n",
        "\n",
        "\n",
        "# Demo: fill 8:00 hour and show remaining availability\n",
        "planner = BookingPlanner()\n",
        "print(\"Initial availability:\", planner.availability_label())\n",
        "assigned = []\n",
        "for i in range(9):\n",
        "    assigned.append(planner.book(8))\n",
        "print(\"Assigned times at 8am:\", assigned)\n",
        "print(\"Availability after filling 8am:\", planner.availability_label())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5d454cfb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bbb5da7"
      },
      "source": [
        "# Optional: Interactive booking widget (Colab)\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, clear_output\n",
        "\n",
        "    planner_widget = BookingPlanner()\n",
        "\n",
        "    def hour_options():\n",
        "        return [(f\"{h}:00 ({planner_widget.capacity_by_hour[h] - planner_widget.booked_count_by_hour[h]} left)\", h)\n",
        "                for h in planner_widget.available_hours()]\n",
        "\n",
        "    hour_dd = widgets.Dropdown(options=hour_options(), description='Hour:')\n",
        "    out = widgets.Output()\n",
        "\n",
        "    def on_book(_):\n",
        "        with out:\n",
        "            clear_output()\n",
        "            if not hour_dd.options:\n",
        "                print(\"No hours available.\")\n",
        "                return\n",
        "            hour = hour_dd.value\n",
        "            assigned_time = planner_widget.book(hour)\n",
        "            if assigned_time is None:\n",
        "                print(\"Selected hour is full. Choose another.\")\n",
        "            else:\n",
        "                print(f\"Booked at {assigned_time}\")\n",
        "                print(\"Remaining availability:\", planner_widget.availability_label())\n",
        "            # refresh dropdown\n",
        "            hour_dd.options = hour_options()\n",
        "\n",
        "    btn = widgets.Button(description='Book')\n",
        "    btn.on_click(on_book)\n",
        "\n",
        "    display(widgets.VBox([hour_dd, btn, out]))\n",
        "except Exception as e:\n",
        "    print(\"Widgets unavailable:\", e)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7bbb5da7"
    }
  ],
  "metadata": {
    "colab": {
      "name": "Clinic Scheduling RL",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}